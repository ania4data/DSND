{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Networks\n",
    "\n",
    "The network we built in the previous part isn't so smart, it doesn't know anything about our handwritten digits. Neural networks with non-linear activations work like universal function approximators. There is some function that maps your input to the output. For example, images of handwritten digits to class probabilities. The power of neural networks is that we can train them to approximate this function, and basically any function given enough data and compute time.\n",
    "\n",
    "<img src=\"assets/function_approx.png\" width=500px>\n",
    "\n",
    "At first the network is naive, it doesn't know the function mapping the inputs to the outputs. We train the network by showing it examples of real data, then adjusting the network parameters such that it approximates this function.\n",
    "\n",
    "To find these parameters, we need to know how poorly the network is predicting the real outputs. For this we calculate a **loss function** (also called the cost), a measure of our prediction error. For example, the mean squared loss is often used in regression and binary classification problems\n",
    "\n",
    "$$\n",
    "\\ell = \\frac{1}{2n}\\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2}\n",
    "$$\n",
    "\n",
    "where $n$ is the number of training examples, $y_i$ are the true labels, and $\\hat{y}_i$ are the predicted labels.\n",
    "\n",
    "By minimizing this loss with respect to the network parameters, we can find configurations where the loss is at a minimum and the network is able to predict the correct labels with high accuracy. We find this minimum using a process called **gradient descent**. The gradient is the slope of the loss function and points in the direction of fastest change. To get to the minimum in the least amount of time, we then want to follow the gradient (downwards). You can think of this like descending a mountain by following the steepest slope to the base.\n",
    "\n",
    "<img src='assets/gradient_descent.png' width=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "For single layer networks, gradient descent is simple to implement. However, it's more complicated for deeper, multilayer neural networks like the one we've built. Complicated enough that it took about 30 years before researchers figured out how to train multilayer networks, although it's straightforward once you learn about it. \n",
    "\n",
    "This is done through **backpropagation** which is really just an application of the chain rule from calculus. It's easiest to understand if we convert a two layer network into a graph representation.\n",
    "\n",
    "<img src='assets/w1_backprop_graph.png' width=400px>\n",
    "\n",
    "In the forward pass through the network, our data and operations go from right to left here. To train the weights with gradient descent, we propagate the gradient of the cost backwards through the network. Mathematically, this is really just calculating the gradient of the loss with respect to the weights using the chain rule.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell}{\\partial w_1} = \\frac{\\partial l_1}{\\partial w_1} \\frac{\\partial s}{\\partial l_1} \\frac{\\partial l_2}{\\partial s} \\frac{\\partial \\ell}{\\partial l_2}\n",
    "$$\n",
    "\n",
    "We update our weights using this gradient with some learning rate $\\alpha$. \n",
    "\n",
    "$$\n",
    "w^\\prime = w - \\alpha \\frac{\\partial \\ell}{\\partial w}\n",
    "$$\n",
    "\n",
    "The learning rate is set such that the weight update steps are small enough that the iterative method settles in a minimum.\n",
    "\n",
    "The first thing we need to do for training is define our loss function. In PyTorch, you'll usually see this as `criterion`. Here we're using softmax output, so we want to use `criterion = nn.CrossEntropyLoss()` as our loss. Later when training, you use `loss = criterion(output, targets)` to calculate the actual loss.\n",
    "\n",
    "We also need to define the optimizer we're using, SGD or Adam, or something along those lines. Here I'll just use SGD with `torch.optim.SGD`, passing in the network parameters and the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "\n",
    "Torch provides a module, `autograd`, for automatically calculating the gradient of tensors. It does this by keeping track of operations performed on tensors. To make sure PyTorch keeps track of operations on a tensor and calculates the gradients, you need to set `requires_grad` on a tensor. You can do this at creation with the `requires_grad` keyword, or at any time with `x.requires_grad_(True)`.\n",
    "\n",
    "You can turn off gradients for a block of code with the `torch.no_grad()` content:\n",
    "```python\n",
    "x = torch.zeros(1, requires_grad=True)\n",
    ">>> with torch.no_grad():\n",
    "...     y = x * 2\n",
    ">>> y.requires_grad\n",
    "False\n",
    "```\n",
    "\n",
    "Also, you can turn on or off gradients altogether with `torch.set_grad_enabled(True|False)`.\n",
    "\n",
    "The gradients are computed with respect to some variable `z` with `z.backward()`. This does a backward pass through the operations that created `z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0020,  0.8200],\n",
      "        [ 0.8813,  1.4125]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0040,  0.6724],\n",
      "        [ 0.7767,  1.9952]])\n"
     ]
    }
   ],
   "source": [
    "y = x**2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see the operation that created `y`, a power operation `PowBackward0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PowBackward0 object at 0x00000189C0B6C128>\n"
     ]
    }
   ],
   "source": [
    "## grad_fn shows the function that generated this variable\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autgrad module keeps track of these operations and knows how to calculate the gradient for each one. In this way, it's able to calculate the gradients for a chain of operations, with respect to any one tensor. Let's reduce the tensor `y` to a scalar value, the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1121)\n"
     ]
    }
   ],
   "source": [
    "z = y.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the gradients for `x` and `y` but they are empty currently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the gradients, you need to run the `.backward` method on a Variable, `z` for example. This will calculate the gradient for `z` with respect to `x`\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial}{\\partial x}\\left[\\frac{1}{n}\\sum_i^n x_i^2\\right] = \\frac{x}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5010,  0.4100],\n",
      "        [ 0.4407,  0.7063]])\n",
      "tensor([[ 0.5010,  0.4100],\n",
      "        [ 0.4407,  0.7063]])\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(x.grad)\n",
    "print(x/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These gradients calculations are particularly useful for neural networks. For training we need the gradients of the weights with respect to the cost. With PyTorch, we run data forward through the network to calculate the cost, then, go backwards to calculate the gradients with respect to the cost. Once we have the gradients we can make a gradient descent step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data and define the network\n",
    "\n",
    "The same as we saw in part 3, we'll load the MNIST dataset and define our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                             ])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll build a network with `nn.Sequential` here. Only difference from the last part is I'm not actually using softmax on the output, but instead just using the raw output from the last layer. This is because the output from softmax is a probability distribution. Often, the output will have values really close to zero or really close to one. Due to [inaccuracies with representing numbers as floating points](https://docs.python.org/3/tutorial/floatingpoint.html), computations with a softmax output can lose accuracy and become unstable. To get around this, we'll use the raw output, called the **logits**, to calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(OrderedDict([\n",
    "                      ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "                      ('relu1', nn.ReLU()),\n",
    "                      ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "                      ('relu2', nn.ReLU()),\n",
    "                      ('logits', nn.Linear(hidden_sizes[1], output_size))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network!\n",
    "\n",
    "The first thing we need to do for training is define our loss function. In PyTorch, you'll usually see this as `criterion`. Here we're using softmax output, so we want to use `criterion = nn.CrossEntropyLoss()` as our loss. Later when training, you use `loss = criterion(output, targets)` to calculate the actual loss.\n",
    "\n",
    "We also need to define the optimizer we're using, SGD or Adam, or something along those lines. Here I'll just use SGD with `torch.optim.SGD`, passing in the network parameters and the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's consider just one learning step before looping through all the data. The general process with PyTorch:\n",
    "\n",
    "* Make a forward pass through the network to get the logits \n",
    "* Use the logits to calculate the loss\n",
    "* Perform a backward pass through the network with `loss.backward()` to calculate the gradients\n",
    "* Take a step with the optimizer to update the weights\n",
    "\n",
    "Below I'll go through one training step and print out the weights and gradients so you can see how it changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights -  Parameter containing:\n",
      "tensor(1.00000e-02 *\n",
      "       [[ 0.0533, -0.4544,  2.7985,  ...,  3.4426,  3.0057, -2.1494],\n",
      "        [ 1.3741,  0.7443, -2.0451,  ...,  1.2414, -1.0602,  2.4259],\n",
      "        [-1.3149,  1.1927, -0.7107,  ...,  3.0063, -1.7802, -0.9433],\n",
      "        ...,\n",
      "        [ 2.3942,  3.4971,  2.9493,  ...,  1.1155, -1.8062, -2.6216],\n",
      "        [-2.3545, -0.1671, -1.0136,  ...,  1.9638,  3.4373, -1.0215],\n",
      "        [ 1.9277, -2.8281,  3.4197,  ...,  1.8021,  1.7838, -3.3088]])\n",
      "Gradient - tensor(1.00000e-02 *\n",
      "       [[ 0.0852,  0.0852,  0.0852,  ...,  0.0852,  0.0852,  0.0852],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1017,  0.1017,  0.1017,  ...,  0.1017,  0.1017,  0.1017],\n",
      "        ...,\n",
      "        [ 0.0185,  0.0185,  0.0185,  ...,  0.0185,  0.0185,  0.0185],\n",
      "        [-0.2097, -0.2097, -0.2097,  ..., -0.2097, -0.2097, -0.2097],\n",
      "        [-0.0527, -0.0527, -0.0527,  ..., -0.0527, -0.0527, -0.0527]])\n"
     ]
    }
   ],
   "source": [
    "print('Initial weights - ', model.fc1.weight)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(64, 784)\n",
    "\n",
    "# Clear the gradients, do this because gradients are accumulated\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass, then backward pass, then update weights\n",
    "output = model.forward(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "print('Gradient -', model.fc1.weight.grad)\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights -  Parameter containing:\n",
      "tensor([[ 5.2451e-04, -4.5529e-03,  2.7976e-02,  ...,  3.4417e-02,\n",
      "          3.0049e-02, -2.1503e-02],\n",
      "        [ 1.3741e-02,  7.4434e-03, -2.0451e-02,  ...,  1.2414e-02,\n",
      "         -1.0602e-02,  2.4259e-02],\n",
      "        [-1.3159e-02,  1.1917e-02, -7.1170e-03,  ...,  3.0052e-02,\n",
      "         -1.7812e-02, -9.4436e-03],\n",
      "        ...,\n",
      "        [ 2.3940e-02,  3.4969e-02,  2.9491e-02,  ...,  1.1154e-02,\n",
      "         -1.8064e-02, -2.6218e-02],\n",
      "        [-2.3524e-02, -1.6500e-03, -1.0115e-02,  ...,  1.9659e-02,\n",
      "          3.4394e-02, -1.0194e-02],\n",
      "        [ 1.9283e-02, -2.8276e-02,  3.4202e-02,  ...,  1.8026e-02,\n",
      "          1.7843e-02, -3.3083e-02]])\n"
     ]
    }
   ],
   "source": [
    "print('Updated weights - ', model.fc1.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training for real\n",
    "\n",
    "Now we'll put this algorithm into a loop so we can go through all the images. This is fairly straightforward. We'll loop through the mini-batches in our dataset, pass the data through the network to calculate the losses, get the gradients, then run the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader._DataLoaderIter object at 0x00000189C9A12438>\n"
     ]
    }
   ],
   "source": [
    "print(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (_, _) in enumerate(trainloader):\n",
    "    print(batch_idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 1/3...  Loss: 0.2742\n",
      "Epoch:2 1/3...  Loss: 0.4754\n",
      "Epoch:3 1/3...  Loss: 0.2641\n",
      "Epoch:4 1/3...  Loss: 0.2626\n",
      "Epoch:5 1/3...  Loss: 0.3155\n",
      "Epoch:6 1/3...  Loss: 0.2109\n",
      "Epoch:7 1/3...  Loss: 0.2895\n",
      "Epoch:8 1/3...  Loss: 0.2697\n",
      "Epoch:9 1/3...  Loss: 0.5049\n",
      "Epoch:10 1/3...  Loss: 0.2139\n",
      "Epoch:11 1/3...  Loss: 0.2941\n",
      "Epoch:12 1/3...  Loss: 0.4196\n",
      "Epoch:13 1/3...  Loss: 0.3254\n",
      "Epoch:14 1/3...  Loss: 0.3308\n",
      "Epoch:15 1/3...  Loss: 0.6238\n",
      "Epoch:16 1/3...  Loss: 0.3669\n",
      "Epoch:17 1/3...  Loss: 0.2385\n",
      "Epoch:18 1/3...  Loss: 0.2612\n",
      "Epoch:19 1/3...  Loss: 0.4396\n",
      "Epoch:20 1/3...  Loss: 0.3613\n",
      "Epoch:21 1/3...  Loss: 0.3944\n",
      "Epoch:22 1/3...  Loss: 0.3264\n",
      "Epoch:23 1/3...  Loss: 0.2510\n",
      "Epoch:24 1/3...  Loss: 0.2542\n",
      "Epoch:25 1/3...  Loss: 0.3868\n",
      "Epoch:26 1/3...  Loss: 0.5476\n",
      "Epoch:27 1/3...  Loss: 0.2287\n",
      "Epoch:28 1/3...  Loss: 0.5222\n",
      "Epoch:29 1/3...  Loss: 0.6173\n",
      "Epoch:30 1/3...  Loss: 0.3248\n",
      "Epoch:31 1/3...  Loss: 0.1589\n",
      "Epoch:32 1/3...  Loss: 0.3713\n",
      "Epoch:33 1/3...  Loss: 0.2873\n",
      "Epoch:34 1/3...  Loss: 0.2722\n",
      "Epoch:35 1/3...  Loss: 0.4109\n",
      "Epoch:36 1/3...  Loss: 0.2637\n",
      "Epoch:37 1/3...  Loss: 0.2718\n",
      "Epoch:38 1/3...  Loss: 0.4101\n",
      "Epoch:39 1/3...  Loss: 0.2175\n",
      "Epoch:40 1/3...  Loss: 0.4326\n",
      "Epoch:41 1/3...  Loss: 0.4123\n",
      "Epoch:42 1/3...  Loss: 0.2788\n",
      "Epoch:43 1/3...  Loss: 0.5651\n",
      "Epoch:44 1/3...  Loss: 0.3015\n",
      "Epoch:45 1/3...  Loss: 0.2723\n",
      "Epoch:46 1/3...  Loss: 0.3684\n",
      "Epoch:47 1/3...  Loss: 0.3012\n",
      "Epoch:48 1/3...  Loss: 0.2773\n",
      "Epoch:49 1/3...  Loss: 0.6539\n",
      "Epoch:50 1/3...  Loss: 0.4770\n",
      "Epoch:51 1/3...  Loss: 0.2607\n",
      "Epoch:52 1/3...  Loss: 0.2983\n",
      "Epoch:53 1/3...  Loss: 0.2265\n",
      "Epoch:54 1/3...  Loss: 0.3716\n",
      "Epoch:55 1/3...  Loss: 0.4250\n",
      "Epoch:56 1/3...  Loss: 0.3227\n",
      "Epoch:57 1/3...  Loss: 0.4353\n",
      "Epoch:58 1/3...  Loss: 0.2966\n",
      "Epoch:59 1/3...  Loss: 0.3533\n",
      "Epoch:60 1/3...  Loss: 0.1593\n",
      "Epoch:61 1/3...  Loss: 0.1842\n",
      "Epoch:62 1/3...  Loss: 0.3618\n",
      "Epoch:63 1/3...  Loss: 0.2964\n",
      "Epoch:64 1/3...  Loss: 0.4478\n",
      "Epoch:65 1/3...  Loss: 0.2741\n",
      "Epoch:66 1/3...  Loss: 0.4060\n",
      "Epoch:67 1/3...  Loss: 0.3238\n",
      "Epoch:68 1/3...  Loss: 0.3990\n",
      "Epoch:69 1/3...  Loss: 0.2525\n",
      "Epoch:70 1/3...  Loss: 0.3698\n",
      "Epoch:71 1/3...  Loss: 0.3256\n",
      "Epoch:72 1/3...  Loss: 0.3862\n",
      "Epoch:73 1/3...  Loss: 0.4403\n",
      "Epoch:74 1/3...  Loss: 0.2330\n",
      "Epoch:75 1/3...  Loss: 0.3580\n",
      "Epoch:76 1/3...  Loss: 0.4081\n",
      "Epoch:77 1/3...  Loss: 0.3451\n",
      "Epoch:78 1/3...  Loss: 0.1755\n",
      "Epoch:79 1/3...  Loss: 0.4638\n",
      "Epoch:80 1/3...  Loss: 0.3793\n",
      "Epoch:81 1/3...  Loss: 0.3971\n",
      "Epoch:82 1/3...  Loss: 0.4026\n",
      "Epoch:83 1/3...  Loss: 0.5161\n",
      "Epoch:84 1/3...  Loss: 0.3504\n",
      "Epoch:85 1/3...  Loss: 0.2834\n",
      "Epoch:86 1/3...  Loss: 0.2378\n",
      "Epoch:87 1/3...  Loss: 0.2509\n",
      "Epoch:88 1/3...  Loss: 0.4875\n",
      "Epoch:89 1/3...  Loss: 0.4086\n",
      "Epoch:90 1/3...  Loss: 0.2499\n",
      "Epoch:91 1/3...  Loss: 0.2903\n",
      "Epoch:92 1/3...  Loss: 0.3218\n",
      "Epoch:93 1/3...  Loss: 0.2475\n",
      "Epoch:94 1/3...  Loss: 0.3877\n",
      "Epoch:95 1/3...  Loss: 0.3312\n",
      "Epoch:96 1/3...  Loss: 0.4125\n",
      "Epoch:97 1/3...  Loss: 0.5083\n",
      "Epoch:98 1/3...  Loss: 0.2507\n",
      "Epoch:99 1/3...  Loss: 0.3882\n",
      "Epoch:100 1/3...  Loss: 0.2827\n",
      "Epoch:101 1/3...  Loss: 0.2727\n",
      "Epoch:102 1/3...  Loss: 0.3665\n",
      "Epoch:103 1/3...  Loss: 0.1720\n",
      "Epoch:104 1/3...  Loss: 0.3719\n",
      "Epoch:105 1/3...  Loss: 0.2571\n",
      "Epoch:106 1/3...  Loss: 0.4809\n",
      "Epoch:107 1/3...  Loss: 0.4176\n",
      "Epoch:108 1/3...  Loss: 0.3127\n",
      "Epoch:109 1/3...  Loss: 0.2669\n",
      "Epoch:110 1/3...  Loss: 0.3542\n",
      "Epoch:111 1/3...  Loss: 0.5039\n",
      "Epoch:112 1/3...  Loss: 0.2976\n",
      "Epoch:113 1/3...  Loss: 0.4293\n",
      "Epoch:114 1/3...  Loss: 0.4318\n",
      "Epoch:115 1/3...  Loss: 0.3246\n",
      "Epoch:116 1/3...  Loss: 0.6528\n",
      "Epoch:117 1/3...  Loss: 0.2442\n",
      "Epoch:118 1/3...  Loss: 0.5026\n",
      "Epoch:119 1/3...  Loss: 0.2479\n",
      "Epoch:120 1/3...  Loss: 0.2419\n",
      "Epoch:121 1/3...  Loss: 0.5018\n",
      "Epoch:122 1/3...  Loss: 0.4601\n",
      "Epoch:123 1/3...  Loss: 0.2248\n",
      "Epoch:124 1/3...  Loss: 0.3370\n",
      "Epoch:125 1/3...  Loss: 0.3910\n",
      "Epoch:126 1/3...  Loss: 0.3072\n",
      "Epoch:127 1/3...  Loss: 0.2842\n",
      "Epoch:128 1/3...  Loss: 0.3358\n",
      "Epoch:129 1/3...  Loss: 0.4384\n",
      "Epoch:130 1/3...  Loss: 0.3425\n",
      "Epoch:131 1/3...  Loss: 0.5722\n",
      "Epoch:132 1/3...  Loss: 0.4089\n",
      "Epoch:133 1/3...  Loss: 0.1159\n",
      "Epoch:134 1/3...  Loss: 0.4658\n",
      "Epoch:135 1/3...  Loss: 0.1907\n",
      "Epoch:136 1/3...  Loss: 0.4401\n",
      "Epoch:137 1/3...  Loss: 0.2766\n",
      "Epoch:138 1/3...  Loss: 0.3615\n",
      "Epoch:139 1/3...  Loss: 0.3401\n",
      "Epoch:140 1/3...  Loss: 0.4394\n",
      "Epoch:141 1/3...  Loss: 0.3343\n",
      "Epoch:142 1/3...  Loss: 0.2548\n",
      "Epoch:143 1/3...  Loss: 0.2353\n",
      "Epoch:144 1/3...  Loss: 0.2886\n",
      "Epoch:145 1/3...  Loss: 0.2399\n",
      "Epoch:146 1/3...  Loss: 0.1850\n",
      "Epoch:147 1/3...  Loss: 0.2549\n",
      "Epoch:148 1/3...  Loss: 0.3458\n",
      "Epoch:149 1/3...  Loss: 0.2895\n",
      "Epoch:150 1/3...  Loss: 0.3640\n",
      "Epoch:151 1/3...  Loss: 0.6388\n",
      "Epoch:152 1/3...  Loss: 0.3929\n",
      "Epoch:153 1/3...  Loss: 0.2731\n",
      "Epoch:154 1/3...  Loss: 0.5028\n",
      "Epoch:155 1/3...  Loss: 0.2749\n",
      "Epoch:156 1/3...  Loss: 0.2554\n",
      "Epoch:157 1/3...  Loss: 0.3267\n",
      "Epoch:158 1/3...  Loss: 0.2602\n",
      "Epoch:159 1/3...  Loss: 0.4625\n",
      "Epoch:160 1/3...  Loss: 0.2611\n",
      "Epoch:161 1/3...  Loss: 0.3163\n",
      "Epoch:162 1/3...  Loss: 0.1731\n",
      "Epoch:163 1/3...  Loss: 0.3818\n",
      "Epoch:164 1/3...  Loss: 0.5447\n",
      "Epoch:165 1/3...  Loss: 0.3115\n",
      "Epoch:166 1/3...  Loss: 0.4827\n",
      "Epoch:167 1/3...  Loss: 0.4178\n",
      "Epoch:168 1/3...  Loss: 0.4036\n",
      "Epoch:169 1/3...  Loss: 0.2574\n",
      "Epoch:170 1/3...  Loss: 0.2741\n",
      "Epoch:171 1/3...  Loss: 0.2102\n",
      "Epoch:172 1/3...  Loss: 0.3383\n",
      "Epoch:173 1/3...  Loss: 0.3282\n",
      "Epoch:174 1/3...  Loss: 0.2131\n",
      "Epoch:175 1/3...  Loss: 0.3764\n",
      "Epoch:176 1/3...  Loss: 0.3549\n",
      "Epoch:177 1/3...  Loss: 0.3868\n",
      "Epoch:178 1/3...  Loss: 0.4013\n",
      "Epoch:179 1/3...  Loss: 0.4599\n",
      "Epoch:180 1/3...  Loss: 0.3618\n",
      "Epoch:181 1/3...  Loss: 0.2490\n",
      "Epoch:182 1/3...  Loss: 0.4039\n",
      "Epoch:183 1/3...  Loss: 0.4155\n",
      "Epoch:184 1/3...  Loss: 0.1955\n",
      "Epoch:185 1/3...  Loss: 0.4974\n",
      "Epoch:186 1/3...  Loss: 0.3824\n",
      "Epoch:187 1/3...  Loss: 0.3596\n",
      "Epoch:188 1/3...  Loss: 0.2365\n",
      "Epoch:189 1/3...  Loss: 0.4687\n",
      "Epoch:190 1/3...  Loss: 0.2416\n",
      "Epoch:191 1/3...  Loss: 0.5339\n",
      "Epoch:192 1/3...  Loss: 0.5186\n",
      "Epoch:193 1/3...  Loss: 0.2576\n",
      "Epoch:194 1/3...  Loss: 0.3832\n",
      "Epoch:195 1/3...  Loss: 0.6145\n",
      "Epoch:196 1/3...  Loss: 0.3251\n",
      "Epoch:197 1/3...  Loss: 0.1401\n",
      "Epoch:198 1/3...  Loss: 0.3562\n",
      "Epoch:199 1/3...  Loss: 0.3754\n",
      "Epoch:200 1/3...  Loss: 0.2261\n",
      "Epoch:201 1/3...  Loss: 0.1832\n",
      "Epoch:202 1/3...  Loss: 0.3091\n",
      "Epoch:203 1/3...  Loss: 0.3401\n",
      "Epoch:204 1/3...  Loss: 0.3233\n",
      "Epoch:205 1/3...  Loss: 0.5811\n",
      "Epoch:206 1/3...  Loss: 0.4027\n",
      "Epoch:207 1/3...  Loss: 0.2795\n",
      "Epoch:208 1/3...  Loss: 0.2352\n",
      "Epoch:209 1/3...  Loss: 0.3607\n",
      "Epoch:210 1/3...  Loss: 0.2749\n",
      "Epoch:211 1/3...  Loss: 0.1702\n",
      "Epoch:212 1/3...  Loss: 0.4560\n",
      "Epoch:213 1/3...  Loss: 0.3682\n",
      "Epoch:214 1/3...  Loss: 0.2040\n",
      "Epoch:215 1/3...  Loss: 0.3363\n",
      "Epoch:216 1/3...  Loss: 0.3843\n",
      "Epoch:217 1/3...  Loss: 0.3457\n",
      "Epoch:218 1/3...  Loss: 0.3276\n",
      "Epoch:219 1/3...  Loss: 0.3839\n",
      "Epoch:220 1/3...  Loss: 0.5135\n",
      "Epoch:221 1/3...  Loss: 0.2078\n",
      "Epoch:222 1/3...  Loss: 0.6999\n",
      "Epoch:223 1/3...  Loss: 0.3827\n",
      "Epoch:224 1/3...  Loss: 0.3193\n",
      "Epoch:225 1/3...  Loss: 0.1950\n",
      "Epoch:226 1/3...  Loss: 0.3421\n",
      "Epoch:227 1/3...  Loss: 0.3953\n",
      "Epoch:228 1/3...  Loss: 0.3404\n",
      "Epoch:229 1/3...  Loss: 0.3709\n",
      "Epoch:230 1/3...  Loss: 0.3056\n",
      "Epoch:231 1/3...  Loss: 0.4809\n",
      "Epoch:232 1/3...  Loss: 0.3410\n",
      "Epoch:233 1/3...  Loss: 0.4041\n",
      "Epoch:234 1/3...  Loss: 0.3392\n",
      "Epoch:235 1/3...  Loss: 0.3790\n",
      "Epoch:236 1/3...  Loss: 0.2871\n",
      "Epoch:237 1/3...  Loss: 0.3467\n",
      "Epoch:238 1/3...  Loss: 0.3479\n",
      "Epoch:239 1/3...  Loss: 0.2067\n",
      "Epoch:240 1/3...  Loss: 0.2895\n",
      "Epoch:241 1/3...  Loss: 0.4266\n",
      "Epoch:242 1/3...  Loss: 0.5236\n",
      "Epoch:243 1/3...  Loss: 0.3931\n",
      "Epoch:244 1/3...  Loss: 0.3544\n",
      "Epoch:245 1/3...  Loss: 0.3360\n",
      "Epoch:246 1/3...  Loss: 0.4766\n",
      "Epoch:247 1/3...  Loss: 0.4944\n",
      "Epoch:248 1/3...  Loss: 0.3903\n",
      "Epoch:249 1/3...  Loss: 0.5481\n",
      "Epoch:250 1/3...  Loss: 0.1172\n",
      "Epoch:251 1/3...  Loss: 0.3462\n",
      "Epoch:252 1/3...  Loss: 0.5593\n",
      "Epoch:253 1/3...  Loss: 0.2247\n",
      "Epoch:254 1/3...  Loss: 0.4266\n",
      "Epoch:255 1/3...  Loss: 0.3354\n",
      "Epoch:256 1/3...  Loss: 0.1717\n",
      "Epoch:257 1/3...  Loss: 0.8037\n",
      "Epoch:258 1/3...  Loss: 0.2269\n",
      "Epoch:259 1/3...  Loss: 0.3324\n",
      "Epoch:260 1/3...  Loss: 0.3479\n",
      "Epoch:261 1/3...  Loss: 0.2282\n",
      "Epoch:262 1/3...  Loss: 0.2677\n",
      "Epoch:263 1/3...  Loss: 0.1826\n",
      "Epoch:264 1/3...  Loss: 0.2322\n",
      "Epoch:265 1/3...  Loss: 0.2920\n",
      "Epoch:266 1/3...  Loss: 0.2955\n",
      "Epoch:267 1/3...  Loss: 0.3001\n",
      "Epoch:268 1/3...  Loss: 0.4770\n",
      "Epoch:269 1/3...  Loss: 0.3307\n",
      "Epoch:270 1/3...  Loss: 0.2776\n",
      "Epoch:271 1/3...  Loss: 0.2261\n",
      "Epoch:272 1/3...  Loss: 0.6838\n",
      "Epoch:273 1/3...  Loss: 0.3640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:274 1/3...  Loss: 0.1558\n",
      "Epoch:275 1/3...  Loss: 0.1718\n",
      "Epoch:276 1/3...  Loss: 0.2615\n",
      "Epoch:277 1/3...  Loss: 0.4259\n",
      "Epoch:278 1/3...  Loss: 0.2833\n",
      "Epoch:279 1/3...  Loss: 0.2188\n",
      "Epoch:280 1/3...  Loss: 0.3043\n",
      "Epoch:281 1/3...  Loss: 0.3109\n",
      "Epoch:282 1/3...  Loss: 0.3495\n",
      "Epoch:283 1/3...  Loss: 0.3775\n",
      "Epoch:284 1/3...  Loss: 0.4894\n",
      "Epoch:285 1/3...  Loss: 0.2111\n",
      "Epoch:286 1/3...  Loss: 0.2647\n",
      "Epoch:287 1/3...  Loss: 0.4162\n",
      "Epoch:288 1/3...  Loss: 0.2254\n",
      "Epoch:289 1/3...  Loss: 0.4621\n",
      "Epoch:290 1/3...  Loss: 0.2174\n",
      "Epoch:291 1/3...  Loss: 0.4905\n",
      "Epoch:292 1/3...  Loss: 0.4184\n",
      "Epoch:293 1/3...  Loss: 0.3135\n",
      "Epoch:294 1/3...  Loss: 0.3119\n",
      "Epoch:295 1/3...  Loss: 0.3678\n",
      "Epoch:296 1/3...  Loss: 0.2850\n",
      "Epoch:297 1/3...  Loss: 0.3029\n",
      "Epoch:298 1/3...  Loss: 0.3205\n",
      "Epoch:299 1/3...  Loss: 0.2954\n",
      "Epoch:300 1/3...  Loss: 0.2838\n",
      "Epoch:301 1/3...  Loss: 0.3334\n",
      "Epoch:302 1/3...  Loss: 0.3785\n",
      "Epoch:303 1/3...  Loss: 0.4811\n",
      "Epoch:304 1/3...  Loss: 0.5735\n",
      "Epoch:305 1/3...  Loss: 0.3751\n",
      "Epoch:306 1/3...  Loss: 0.4048\n",
      "Epoch:307 1/3...  Loss: 0.3156\n",
      "Epoch:308 1/3...  Loss: 0.2272\n",
      "Epoch:309 1/3...  Loss: 0.3447\n",
      "Epoch:310 1/3...  Loss: 0.3527\n",
      "Epoch:311 1/3...  Loss: 0.1779\n",
      "Epoch:312 1/3...  Loss: 0.1377\n",
      "Epoch:313 1/3...  Loss: 0.3264\n",
      "Epoch:314 1/3...  Loss: 0.2219\n",
      "Epoch:315 1/3...  Loss: 0.3116\n",
      "Epoch:316 1/3...  Loss: 0.4606\n",
      "Epoch:317 1/3...  Loss: 0.2642\n",
      "Epoch:318 1/3...  Loss: 0.4567\n",
      "Epoch:319 1/3...  Loss: 0.2567\n",
      "Epoch:320 1/3...  Loss: 0.4203\n",
      "Epoch:321 1/3...  Loss: 0.5535\n",
      "Epoch:322 1/3...  Loss: 0.2730\n",
      "Epoch:323 1/3...  Loss: 0.1901\n",
      "Epoch:324 1/3...  Loss: 0.3166\n",
      "Epoch:325 1/3...  Loss: 0.4370\n",
      "Epoch:326 1/3...  Loss: 0.3315\n",
      "Epoch:327 1/3...  Loss: 0.4044\n",
      "Epoch:328 1/3...  Loss: 0.2279\n",
      "Epoch:329 1/3...  Loss: 0.2914\n",
      "Epoch:330 1/3...  Loss: 0.1415\n",
      "Epoch:331 1/3...  Loss: 0.3401\n",
      "Epoch:332 1/3...  Loss: 0.1999\n",
      "Epoch:333 1/3...  Loss: 0.3348\n",
      "Epoch:334 1/3...  Loss: 0.4963\n",
      "Epoch:335 1/3...  Loss: 0.3542\n",
      "Epoch:336 1/3...  Loss: 0.3341\n",
      "Epoch:337 1/3...  Loss: 0.2764\n",
      "Epoch:338 1/3...  Loss: 0.2438\n",
      "Epoch:339 1/3...  Loss: 0.3936\n",
      "Epoch:340 1/3...  Loss: 0.2060\n",
      "Epoch:341 1/3...  Loss: 0.2598\n",
      "Epoch:342 1/3...  Loss: 0.2224\n",
      "Epoch:343 1/3...  Loss: 0.2532\n",
      "Epoch:344 1/3...  Loss: 0.4609\n",
      "Epoch:345 1/3...  Loss: 0.2427\n",
      "Epoch:346 1/3...  Loss: 0.3317\n",
      "Epoch:347 1/3...  Loss: 0.4135\n",
      "Epoch:348 1/3...  Loss: 0.5027\n",
      "Epoch:349 1/3...  Loss: 0.2276\n",
      "Epoch:350 1/3...  Loss: 0.5056\n",
      "Epoch:351 1/3...  Loss: 0.3047\n",
      "Epoch:352 1/3...  Loss: 0.4499\n",
      "Epoch:353 1/3...  Loss: 0.4016\n",
      "Epoch:354 1/3...  Loss: 0.4109\n",
      "Epoch:355 1/3...  Loss: 0.3067\n",
      "Epoch:356 1/3...  Loss: 0.3142\n",
      "Epoch:357 1/3...  Loss: 0.3942\n",
      "Epoch:358 1/3...  Loss: 0.3098\n",
      "Epoch:359 1/3...  Loss: 0.3135\n",
      "Epoch:360 1/3...  Loss: 0.4207\n",
      "Epoch:361 1/3...  Loss: 0.2631\n",
      "Epoch:362 1/3...  Loss: 0.2439\n",
      "Epoch:363 1/3...  Loss: 0.4471\n",
      "Epoch:364 1/3...  Loss: 0.2244\n",
      "Epoch:365 1/3...  Loss: 0.5031\n",
      "Epoch:366 1/3...  Loss: 0.3537\n",
      "Epoch:367 1/3...  Loss: 0.3108\n",
      "Epoch:368 1/3...  Loss: 0.2503\n",
      "Epoch:369 1/3...  Loss: 0.2752\n",
      "Epoch:370 1/3...  Loss: 0.4924\n",
      "Epoch:371 1/3...  Loss: 0.3900\n",
      "Epoch:372 1/3...  Loss: 0.2397\n",
      "Epoch:373 1/3...  Loss: 0.2695\n",
      "Epoch:374 1/3...  Loss: 0.3183\n",
      "Epoch:375 1/3...  Loss: 0.1809\n",
      "Epoch:376 1/3...  Loss: 0.2200\n",
      "Epoch:377 1/3...  Loss: 0.3124\n",
      "Epoch:378 1/3...  Loss: 0.2404\n",
      "Epoch:379 1/3...  Loss: 0.4602\n",
      "Epoch:380 1/3...  Loss: 0.3040\n",
      "Epoch:381 1/3...  Loss: 0.3010\n",
      "Epoch:382 1/3...  Loss: 0.3252\n",
      "Epoch:383 1/3...  Loss: 0.4606\n",
      "Epoch:384 1/3...  Loss: 0.2863\n",
      "Epoch:385 1/3...  Loss: 0.4597\n",
      "Epoch:386 1/3...  Loss: 0.2386\n",
      "Epoch:387 1/3...  Loss: 0.3164\n",
      "Epoch:388 1/3...  Loss: 0.2922\n",
      "Epoch:389 1/3...  Loss: 0.3371\n",
      "Epoch:390 1/3...  Loss: 0.3606\n",
      "Epoch:391 1/3...  Loss: 0.4568\n",
      "Epoch:392 1/3...  Loss: 0.3526\n",
      "Epoch:393 1/3...  Loss: 0.4921\n",
      "Epoch:394 1/3...  Loss: 0.2369\n",
      "Epoch:395 1/3...  Loss: 0.3557\n",
      "Epoch:396 1/3...  Loss: 0.2473\n",
      "Epoch:397 1/3...  Loss: 0.3867\n",
      "Epoch:398 1/3...  Loss: 0.4257\n",
      "Epoch:399 1/3...  Loss: 0.5165\n",
      "Epoch:400 1/3...  Loss: 0.2548\n",
      "Epoch:401 1/3...  Loss: 0.1891\n",
      "Epoch:402 1/3...  Loss: 0.6571\n",
      "Epoch:403 1/3...  Loss: 0.4025\n",
      "Epoch:404 1/3...  Loss: 0.2488\n",
      "Epoch:405 1/3...  Loss: 0.1351\n",
      "Epoch:406 1/3...  Loss: 0.2117\n",
      "Epoch:407 1/3...  Loss: 0.5102\n",
      "Epoch:408 1/3...  Loss: 0.4692\n",
      "Epoch:409 1/3...  Loss: 0.4428\n",
      "Epoch:410 1/3...  Loss: 0.2807\n",
      "Epoch:411 1/3...  Loss: 0.4977\n",
      "Epoch:412 1/3...  Loss: 0.2199\n",
      "Epoch:413 1/3...  Loss: 0.3577\n",
      "Epoch:414 1/3...  Loss: 0.2302\n",
      "Epoch:415 1/3...  Loss: 0.5420\n",
      "Epoch:416 1/3...  Loss: 0.3872\n",
      "Epoch:417 1/3...  Loss: 0.2138\n",
      "Epoch:418 1/3...  Loss: 0.3460\n",
      "Epoch:419 1/3...  Loss: 0.2399\n",
      "Epoch:420 1/3...  Loss: 0.2618\n",
      "Epoch:421 1/3...  Loss: 0.2928\n",
      "Epoch:422 1/3...  Loss: 0.2496\n",
      "Epoch:423 1/3...  Loss: 0.2000\n",
      "Epoch:424 1/3...  Loss: 0.1839\n",
      "Epoch:425 1/3...  Loss: 0.3119\n",
      "Epoch:426 1/3...  Loss: 0.5390\n",
      "Epoch:427 1/3...  Loss: 0.2595\n",
      "Epoch:428 1/3...  Loss: 0.2023\n",
      "Epoch:429 1/3...  Loss: 0.2849\n",
      "Epoch:430 1/3...  Loss: 0.3548\n",
      "Epoch:431 1/3...  Loss: 0.2904\n",
      "Epoch:432 1/3...  Loss: 0.3409\n",
      "Epoch:433 1/3...  Loss: 0.3675\n",
      "Epoch:434 1/3...  Loss: 0.3057\n",
      "Epoch:435 1/3...  Loss: 0.2692\n",
      "Epoch:436 1/3...  Loss: 0.4731\n",
      "Epoch:437 1/3...  Loss: 0.2832\n",
      "Epoch:438 1/3...  Loss: 0.2953\n",
      "Epoch:439 1/3...  Loss: 0.2040\n",
      "Epoch:440 1/3...  Loss: 0.4696\n",
      "Epoch:441 1/3...  Loss: 0.2619\n",
      "Epoch:442 1/3...  Loss: 0.4196\n",
      "Epoch:443 1/3...  Loss: 0.1618\n",
      "Epoch:444 1/3...  Loss: 0.2224\n",
      "Epoch:445 1/3...  Loss: 0.3828\n",
      "Epoch:446 1/3...  Loss: 0.2218\n",
      "Epoch:447 1/3...  Loss: 0.3269\n",
      "Epoch:448 1/3...  Loss: 0.4114\n",
      "Epoch:449 1/3...  Loss: 0.3003\n",
      "Epoch:450 1/3...  Loss: 0.2878\n",
      "Epoch:451 1/3...  Loss: 0.2341\n",
      "Epoch:452 1/3...  Loss: 0.5720\n",
      "Epoch:453 1/3...  Loss: 0.4097\n",
      "Epoch:454 1/3...  Loss: 0.3462\n",
      "Epoch:455 1/3...  Loss: 0.2331\n",
      "Epoch:456 1/3...  Loss: 0.2006\n",
      "Epoch:457 1/3...  Loss: 0.4109\n",
      "Epoch:458 1/3...  Loss: 0.5220\n",
      "Epoch:459 1/3...  Loss: 0.4638\n",
      "Epoch:460 1/3...  Loss: 0.2813\n",
      "Epoch:461 1/3...  Loss: 0.6113\n",
      "Epoch:462 1/3...  Loss: 0.2537\n",
      "Epoch:463 1/3...  Loss: 0.2546\n",
      "Epoch:464 1/3...  Loss: 0.3251\n",
      "Epoch:465 1/3...  Loss: 0.3425\n",
      "Epoch:466 1/3...  Loss: 0.2779\n",
      "Epoch:467 1/3...  Loss: 0.2319\n",
      "Epoch:468 1/3...  Loss: 0.4414\n",
      "Epoch:469 1/3...  Loss: 0.8264\n",
      "Epoch:470 1/3...  Loss: 0.2307\n",
      "Epoch:471 1/3...  Loss: 0.2588\n",
      "Epoch:472 1/3...  Loss: 0.2902\n",
      "Epoch:473 1/3...  Loss: 0.3507\n",
      "Epoch:474 1/3...  Loss: 0.2395\n",
      "Epoch:475 1/3...  Loss: 0.3279\n",
      "Epoch:476 1/3...  Loss: 0.3579\n",
      "Epoch:477 1/3...  Loss: 0.2485\n",
      "Epoch:478 1/3...  Loss: 0.2624\n",
      "Epoch:479 1/3...  Loss: 0.2432\n",
      "Epoch:480 1/3...  Loss: 0.2571\n",
      "Epoch:481 1/3...  Loss: 0.3720\n",
      "Epoch:482 1/3...  Loss: 0.2396\n",
      "Epoch:483 1/3...  Loss: 0.2433\n",
      "Epoch:484 1/3...  Loss: 0.5237\n",
      "Epoch:485 1/3...  Loss: 0.5648\n",
      "Epoch:486 1/3...  Loss: 0.4252\n",
      "Epoch:487 1/3...  Loss: 0.1457\n",
      "Epoch:488 1/3...  Loss: 0.2149\n",
      "Epoch:489 1/3...  Loss: 0.2660\n",
      "Epoch:490 1/3...  Loss: 0.4229\n",
      "Epoch:491 1/3...  Loss: 0.2993\n",
      "Epoch:492 1/3...  Loss: 0.3293\n",
      "Epoch:493 1/3...  Loss: 0.2265\n",
      "Epoch:494 1/3...  Loss: 0.2735\n",
      "Epoch:495 1/3...  Loss: 0.3494\n",
      "Epoch:496 1/3...  Loss: 0.3779\n",
      "Epoch:497 1/3...  Loss: 0.3485\n",
      "Epoch:498 1/3...  Loss: 0.2846\n",
      "Epoch:499 1/3...  Loss: 0.3487\n",
      "Epoch:500 1/3...  Loss: 0.2319\n",
      "Epoch:501 1/3...  Loss: 0.5051\n",
      "Epoch:502 1/3...  Loss: 0.2814\n",
      "Epoch:503 1/3...  Loss: 0.4499\n",
      "Epoch:504 1/3...  Loss: 0.3202\n",
      "Epoch:505 1/3...  Loss: 0.2033\n",
      "Epoch:506 1/3...  Loss: 0.2952\n",
      "Epoch:507 1/3...  Loss: 0.1454\n",
      "Epoch:508 1/3...  Loss: 0.3608\n",
      "Epoch:509 1/3...  Loss: 0.2880\n",
      "Epoch:510 1/3...  Loss: 0.2880\n",
      "Epoch:511 1/3...  Loss: 0.2663\n",
      "Epoch:512 1/3...  Loss: 0.4100\n",
      "Epoch:513 1/3...  Loss: 0.4199\n",
      "Epoch:514 1/3...  Loss: 0.3150\n",
      "Epoch:515 1/3...  Loss: 0.3455\n",
      "Epoch:516 1/3...  Loss: 0.3758\n",
      "Epoch:517 1/3...  Loss: 0.3698\n",
      "Epoch:518 1/3...  Loss: 0.2526\n",
      "Epoch:519 1/3...  Loss: 0.1807\n",
      "Epoch:520 1/3...  Loss: 0.2497\n",
      "Epoch:521 1/3...  Loss: 0.2930\n",
      "Epoch:522 1/3...  Loss: 0.4042\n",
      "Epoch:523 1/3...  Loss: 0.4226\n",
      "Epoch:524 1/3...  Loss: 0.3690\n",
      "Epoch:525 1/3...  Loss: 0.3223\n",
      "Epoch:526 1/3...  Loss: 0.4269\n",
      "Epoch:527 1/3...  Loss: 0.3410\n",
      "Epoch:528 1/3...  Loss: 0.5227\n",
      "Epoch:529 1/3...  Loss: 0.4271\n",
      "Epoch:530 1/3...  Loss: 0.3538\n",
      "Epoch:531 1/3...  Loss: 0.3110\n",
      "Epoch:532 1/3...  Loss: 0.3915\n",
      "Epoch:533 1/3...  Loss: 0.4737\n",
      "Epoch:534 1/3...  Loss: 0.3249\n",
      "Epoch:535 1/3...  Loss: 0.3166\n",
      "Epoch:536 1/3...  Loss: 0.3293\n",
      "Epoch:537 1/3...  Loss: 0.4092\n",
      "Epoch:538 1/3...  Loss: 0.5060\n",
      "Epoch:539 1/3...  Loss: 0.3737\n",
      "Epoch:540 1/3...  Loss: 0.5146\n",
      "Epoch:541 1/3...  Loss: 0.2800\n",
      "Epoch:542 1/3...  Loss: 0.1786\n",
      "Epoch:543 1/3...  Loss: 0.3234\n",
      "Epoch:544 1/3...  Loss: 0.2115\n",
      "Epoch:545 1/3...  Loss: 0.3629\n",
      "Epoch:546 1/3...  Loss: 0.5665\n",
      "Epoch:547 1/3...  Loss: 0.2336\n",
      "Epoch:548 1/3...  Loss: 0.3659\n",
      "Epoch:549 1/3...  Loss: 0.4855\n",
      "Epoch:550 1/3...  Loss: 0.3184\n",
      "Epoch:551 1/3...  Loss: 0.2260\n",
      "Epoch:552 1/3...  Loss: 0.2606\n",
      "Epoch:553 1/3...  Loss: 0.4097\n",
      "Epoch:554 1/3...  Loss: 0.3490\n",
      "Epoch:555 1/3...  Loss: 0.2922\n",
      "Epoch:556 1/3...  Loss: 0.5416\n",
      "Epoch:557 1/3...  Loss: 0.2490\n",
      "Epoch:558 1/3...  Loss: 0.2122\n",
      "Epoch:559 1/3...  Loss: 0.3130\n",
      "Epoch:560 1/3...  Loss: 0.2502\n",
      "Epoch:561 1/3...  Loss: 0.3254\n",
      "Epoch:562 1/3...  Loss: 0.2993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:563 1/3...  Loss: 0.2679\n",
      "Epoch:564 1/3...  Loss: 0.3683\n",
      "Epoch:565 1/3...  Loss: 0.5151\n",
      "Epoch:566 1/3...  Loss: 0.3256\n",
      "Epoch:567 1/3...  Loss: 0.2647\n",
      "Epoch:568 1/3...  Loss: 0.3687\n",
      "Epoch:569 1/3...  Loss: 0.4472\n",
      "Epoch:570 1/3...  Loss: 0.4626\n",
      "Epoch:571 1/3...  Loss: 0.3563\n",
      "Epoch:572 1/3...  Loss: 0.3432\n",
      "Epoch:573 1/3...  Loss: 0.5613\n",
      "Epoch:574 1/3...  Loss: 0.5000\n",
      "Epoch:575 1/3...  Loss: 0.3370\n",
      "Epoch:576 1/3...  Loss: 0.6749\n",
      "Epoch:577 1/3...  Loss: 0.2651\n",
      "Epoch:578 1/3...  Loss: 0.1887\n",
      "Epoch:579 1/3...  Loss: 0.3107\n",
      "Epoch:580 1/3...  Loss: 0.4387\n",
      "Epoch:581 1/3...  Loss: 0.2844\n",
      "Epoch:582 1/3...  Loss: 0.2916\n",
      "Epoch:583 1/3...  Loss: 0.3545\n",
      "Epoch:584 1/3...  Loss: 0.4769\n",
      "Epoch:585 1/3...  Loss: 0.3870\n",
      "Epoch:586 1/3...  Loss: 0.3840\n",
      "Epoch:587 1/3...  Loss: 0.5150\n",
      "Epoch:588 1/3...  Loss: 0.4589\n",
      "Epoch:589 1/3...  Loss: 0.3699\n",
      "Epoch:590 1/3...  Loss: 0.3365\n",
      "Epoch:591 1/3...  Loss: 0.2685\n",
      "Epoch:592 1/3...  Loss: 0.2156\n",
      "Epoch:593 1/3...  Loss: 0.3522\n",
      "Epoch:594 1/3...  Loss: 0.6202\n",
      "Epoch:595 1/3...  Loss: 0.3704\n",
      "Epoch:596 1/3...  Loss: 0.2845\n",
      "Epoch:597 1/3...  Loss: 0.4314\n",
      "Epoch:598 1/3...  Loss: 0.4474\n",
      "Epoch:599 1/3...  Loss: 0.2807\n",
      "Epoch:600 1/3...  Loss: 0.2419\n",
      "Epoch:601 1/3...  Loss: 0.1867\n",
      "Epoch:602 1/3...  Loss: 0.3934\n",
      "Epoch:603 1/3...  Loss: 0.2647\n",
      "Epoch:604 1/3...  Loss: 0.1975\n",
      "Epoch:605 1/3...  Loss: 0.2722\n",
      "Epoch:606 1/3...  Loss: 0.2498\n",
      "Epoch:607 1/3...  Loss: 0.3098\n",
      "Epoch:608 1/3...  Loss: 0.1875\n",
      "Epoch:609 1/3...  Loss: 0.2481\n",
      "Epoch:610 1/3...  Loss: 0.4882\n",
      "Epoch:611 1/3...  Loss: 0.3649\n",
      "Epoch:612 1/3...  Loss: 0.2781\n",
      "Epoch:613 1/3...  Loss: 0.2771\n",
      "Epoch:614 1/3...  Loss: 0.3694\n",
      "Epoch:615 1/3...  Loss: 0.2836\n",
      "Epoch:616 1/3...  Loss: 0.2896\n",
      "Epoch:617 1/3...  Loss: 0.2508\n",
      "Epoch:618 1/3...  Loss: 0.4370\n",
      "Epoch:619 1/3...  Loss: 0.3062\n",
      "Epoch:620 1/3...  Loss: 0.3546\n",
      "Epoch:621 1/3...  Loss: 0.5172\n",
      "Epoch:622 1/3...  Loss: 0.4290\n",
      "Epoch:623 1/3...  Loss: 0.2337\n",
      "Epoch:624 1/3...  Loss: 0.3429\n",
      "Epoch:625 1/3...  Loss: 0.3519\n",
      "Epoch:626 1/3...  Loss: 0.3600\n",
      "Epoch:627 1/3...  Loss: 0.2633\n",
      "Epoch:628 1/3...  Loss: 0.1345\n",
      "Epoch:629 1/3...  Loss: 0.5623\n",
      "Epoch:630 1/3...  Loss: 0.2169\n",
      "Epoch:631 1/3...  Loss: 0.2931\n",
      "Epoch:632 1/3...  Loss: 0.3021\n",
      "Epoch:633 1/3...  Loss: 0.4283\n",
      "Epoch:634 1/3...  Loss: 0.1624\n",
      "Epoch:635 1/3...  Loss: 0.5487\n",
      "Epoch:636 1/3...  Loss: 0.3582\n",
      "Epoch:637 1/3...  Loss: 0.3327\n",
      "Epoch:638 1/3...  Loss: 0.3831\n",
      "Epoch:639 1/3...  Loss: 0.1923\n",
      "Epoch:640 1/3...  Loss: 0.3863\n",
      "Epoch:641 1/3...  Loss: 0.1893\n",
      "Epoch:642 1/3...  Loss: 0.4405\n",
      "Epoch:643 1/3...  Loss: 0.3542\n",
      "Epoch:644 1/3...  Loss: 0.2721\n",
      "Epoch:645 1/3...  Loss: 0.2543\n",
      "Epoch:646 1/3...  Loss: 0.4008\n",
      "Epoch:647 1/3...  Loss: 0.1297\n",
      "Epoch:648 1/3...  Loss: 0.6348\n",
      "Epoch:649 1/3...  Loss: 0.4108\n",
      "Epoch:650 1/3...  Loss: 0.3014\n",
      "Epoch:651 1/3...  Loss: 0.4736\n",
      "Epoch:652 1/3...  Loss: 0.2695\n",
      "Epoch:653 1/3...  Loss: 0.3257\n",
      "Epoch:654 1/3...  Loss: 0.3053\n",
      "Epoch:655 1/3...  Loss: 0.2689\n",
      "Epoch:656 1/3...  Loss: 0.5388\n",
      "Epoch:657 1/3...  Loss: 0.3714\n",
      "Epoch:658 1/3...  Loss: 0.2577\n",
      "Epoch:659 1/3...  Loss: 0.2093\n",
      "Epoch:660 1/3...  Loss: 0.3188\n",
      "Epoch:661 1/3...  Loss: 0.5000\n",
      "Epoch:662 1/3...  Loss: 0.6069\n",
      "Epoch:663 1/3...  Loss: 0.3181\n",
      "Epoch:664 1/3...  Loss: 0.4384\n",
      "Epoch:665 1/3...  Loss: 0.2058\n",
      "Epoch:666 1/3...  Loss: 0.3000\n",
      "Epoch:667 1/3...  Loss: 0.3215\n",
      "Epoch:668 1/3...  Loss: 0.2553\n",
      "Epoch:669 1/3...  Loss: 0.3271\n",
      "Epoch:670 1/3...  Loss: 0.3301\n",
      "Epoch:671 1/3...  Loss: 0.4668\n",
      "Epoch:672 1/3...  Loss: 0.3478\n",
      "Epoch:673 1/3...  Loss: 0.3741\n",
      "Epoch:674 1/3...  Loss: 0.2978\n",
      "Epoch:675 1/3...  Loss: 0.2559\n",
      "Epoch:676 1/3...  Loss: 0.3891\n",
      "Epoch:677 1/3...  Loss: 0.4342\n",
      "Epoch:678 1/3...  Loss: 0.4173\n",
      "Epoch:679 1/3...  Loss: 0.4234\n",
      "Epoch:680 1/3...  Loss: 0.3730\n",
      "Epoch:681 1/3...  Loss: 0.4769\n",
      "Epoch:682 1/3...  Loss: 0.3784\n",
      "Epoch:683 1/3...  Loss: 0.2399\n",
      "Epoch:684 1/3...  Loss: 0.3790\n",
      "Epoch:685 1/3...  Loss: 0.3398\n",
      "Epoch:686 1/3...  Loss: 0.1844\n",
      "Epoch:687 1/3...  Loss: 0.3132\n",
      "Epoch:688 1/3...  Loss: 0.2625\n",
      "Epoch:689 1/3...  Loss: 0.3406\n",
      "Epoch:690 1/3...  Loss: 0.3623\n",
      "Epoch:691 1/3...  Loss: 0.4081\n",
      "Epoch:692 1/3...  Loss: 0.4097\n",
      "Epoch:693 1/3...  Loss: 0.2406\n",
      "Epoch:694 1/3...  Loss: 0.2596\n",
      "Epoch:695 1/3...  Loss: 0.3101\n",
      "Epoch:696 1/3...  Loss: 0.2009\n",
      "Epoch:697 1/3...  Loss: 0.3373\n",
      "Epoch:698 1/3...  Loss: 0.4844\n",
      "Epoch:699 1/3...  Loss: 0.4346\n",
      "Epoch:700 1/3...  Loss: 0.4148\n",
      "Epoch:701 1/3...  Loss: 0.4141\n",
      "Epoch:702 1/3...  Loss: 0.1998\n",
      "Epoch:703 1/3...  Loss: 0.4209\n",
      "Epoch:704 1/3...  Loss: 0.5361\n",
      "Epoch:705 1/3...  Loss: 0.1736\n",
      "Epoch:706 1/3...  Loss: 0.2251\n",
      "Epoch:707 1/3...  Loss: 0.4034\n",
      "Epoch:708 1/3...  Loss: 0.2727\n",
      "Epoch:709 1/3...  Loss: 0.5398\n",
      "Epoch:710 1/3...  Loss: 0.4283\n",
      "Epoch:711 1/3...  Loss: 0.3840\n",
      "Epoch:712 1/3...  Loss: 0.3611\n",
      "Epoch:713 1/3...  Loss: 0.5135\n",
      "Epoch:714 1/3...  Loss: 0.2774\n",
      "Epoch:715 1/3...  Loss: 0.3631\n",
      "Epoch:716 1/3...  Loss: 0.4312\n",
      "Epoch:717 1/3...  Loss: 0.3207\n",
      "Epoch:718 1/3...  Loss: 0.4180\n",
      "Epoch:719 1/3...  Loss: 0.2067\n",
      "Epoch:720 1/3...  Loss: 0.3014\n",
      "Epoch:721 1/3...  Loss: 0.2370\n",
      "Epoch:722 1/3...  Loss: 0.1865\n",
      "Epoch:723 1/3...  Loss: 0.5262\n",
      "Epoch:724 1/3...  Loss: 0.2887\n",
      "Epoch:725 1/3...  Loss: 0.2391\n",
      "Epoch:726 1/3...  Loss: 0.2752\n",
      "Epoch:727 1/3...  Loss: 0.3012\n",
      "Epoch:728 1/3...  Loss: 0.2775\n",
      "Epoch:729 1/3...  Loss: 0.2893\n",
      "Epoch:730 1/3...  Loss: 0.2202\n",
      "Epoch:731 1/3...  Loss: 0.3088\n",
      "Epoch:732 1/3...  Loss: 0.3549\n",
      "Epoch:733 1/3...  Loss: 0.4420\n",
      "Epoch:734 1/3...  Loss: 0.2513\n",
      "Epoch:735 1/3...  Loss: 0.3456\n",
      "Epoch:736 1/3...  Loss: 0.6579\n",
      "Epoch:737 1/3...  Loss: 0.2890\n",
      "Epoch:738 1/3...  Loss: 0.1557\n",
      "Epoch:739 1/3...  Loss: 0.4087\n",
      "Epoch:740 1/3...  Loss: 0.2841\n",
      "Epoch:741 1/3...  Loss: 0.3664\n",
      "Epoch:742 1/3...  Loss: 0.3167\n",
      "Epoch:743 1/3...  Loss: 0.4637\n",
      "Epoch:744 1/3...  Loss: 0.4452\n",
      "Epoch:745 1/3...  Loss: 0.4758\n",
      "Epoch:746 1/3...  Loss: 0.5363\n",
      "Epoch:747 1/3...  Loss: 0.2527\n",
      "Epoch:748 1/3...  Loss: 0.5570\n",
      "Epoch:749 1/3...  Loss: 0.4335\n",
      "Epoch:750 1/3...  Loss: 0.2725\n",
      "Epoch:751 1/3...  Loss: 0.3455\n",
      "Epoch:752 1/3...  Loss: 0.3318\n",
      "Epoch:753 1/3...  Loss: 0.2606\n",
      "Epoch:754 1/3...  Loss: 0.2766\n",
      "Epoch:755 1/3...  Loss: 0.2948\n",
      "Epoch:756 1/3...  Loss: 0.3682\n",
      "Epoch:757 1/3...  Loss: 0.2146\n",
      "Epoch:758 1/3...  Loss: 0.3844\n",
      "Epoch:759 1/3...  Loss: 0.2060\n",
      "Epoch:760 1/3...  Loss: 0.0989\n",
      "Epoch:761 1/3...  Loss: 0.4895\n",
      "Epoch:762 1/3...  Loss: 0.4295\n",
      "Epoch:763 1/3...  Loss: 0.3379\n",
      "Epoch:764 1/3...  Loss: 0.2337\n",
      "Epoch:765 1/3...  Loss: 0.2732\n",
      "Epoch:766 1/3...  Loss: 0.3644\n",
      "Epoch:767 1/3...  Loss: 0.2508\n",
      "Epoch:768 1/3...  Loss: 0.3720\n",
      "Epoch:769 1/3...  Loss: 0.4088\n",
      "Epoch:770 1/3...  Loss: 0.3077\n",
      "Epoch:771 1/3...  Loss: 0.3993\n",
      "Epoch:772 1/3...  Loss: 0.2611\n",
      "Epoch:773 1/3...  Loss: 0.1853\n",
      "Epoch:774 1/3...  Loss: 0.1841\n",
      "Epoch:775 1/3...  Loss: 0.3473\n",
      "Epoch:776 1/3...  Loss: 0.4695\n",
      "Epoch:777 1/3...  Loss: 0.2674\n",
      "Epoch:778 1/3...  Loss: 0.3327\n",
      "Epoch:779 1/3...  Loss: 0.2663\n",
      "Epoch:780 1/3...  Loss: 0.3681\n",
      "Epoch:781 1/3...  Loss: 0.2685\n",
      "Epoch:782 1/3...  Loss: 0.2983\n",
      "Epoch:783 1/3...  Loss: 0.2312\n",
      "Epoch:784 1/3...  Loss: 0.1544\n",
      "Epoch:785 1/3...  Loss: 0.4064\n",
      "Epoch:786 1/3...  Loss: 0.2538\n",
      "Epoch:787 1/3...  Loss: 0.3376\n",
      "Epoch:788 1/3...  Loss: 0.2075\n",
      "Epoch:789 1/3...  Loss: 0.1764\n",
      "Epoch:790 1/3...  Loss: 0.3955\n",
      "Epoch:791 1/3...  Loss: 0.1996\n",
      "Epoch:792 1/3...  Loss: 0.3520\n",
      "Epoch:793 1/3...  Loss: 0.2133\n",
      "Epoch:794 1/3...  Loss: 0.2225\n",
      "Epoch:795 1/3...  Loss: 0.3422\n",
      "Epoch:796 1/3...  Loss: 0.5710\n",
      "Epoch:797 1/3...  Loss: 0.2328\n",
      "Epoch:798 1/3...  Loss: 0.2861\n",
      "Epoch:799 1/3...  Loss: 0.4080\n",
      "Epoch:800 1/3...  Loss: 0.3270\n",
      "Epoch:801 1/3...  Loss: 0.5381\n",
      "Epoch:802 1/3...  Loss: 0.2876\n",
      "Epoch:803 1/3...  Loss: 0.2348\n",
      "Epoch:804 1/3...  Loss: 0.2671\n",
      "Epoch:805 1/3...  Loss: 0.4381\n",
      "Epoch:806 1/3...  Loss: 0.3178\n",
      "Epoch:807 1/3...  Loss: 0.2806\n",
      "Epoch:808 1/3...  Loss: 0.4262\n",
      "Epoch:809 1/3...  Loss: 0.1576\n",
      "Epoch:810 1/3...  Loss: 0.2446\n",
      "Epoch:811 1/3...  Loss: 0.4984\n",
      "Epoch:812 1/3...  Loss: 0.4102\n",
      "Epoch:813 1/3...  Loss: 0.2061\n",
      "Epoch:814 1/3...  Loss: 0.4048\n",
      "Epoch:815 1/3...  Loss: 0.2856\n",
      "Epoch:816 1/3...  Loss: 0.2220\n",
      "Epoch:817 1/3...  Loss: 0.4228\n",
      "Epoch:818 1/3...  Loss: 0.3316\n",
      "Epoch:819 1/3...  Loss: 0.2728\n",
      "Epoch:820 1/3...  Loss: 0.5055\n",
      "Epoch:821 1/3...  Loss: 0.4076\n",
      "Epoch:822 1/3...  Loss: 0.4248\n",
      "Epoch:823 1/3...  Loss: 0.4508\n",
      "Epoch:824 1/3...  Loss: 0.1977\n",
      "Epoch:825 1/3...  Loss: 0.2740\n",
      "Epoch:826 1/3...  Loss: 0.5022\n",
      "Epoch:827 1/3...  Loss: 0.2286\n",
      "Epoch:828 1/3...  Loss: 0.1332\n",
      "Epoch:829 1/3...  Loss: 0.2909\n",
      "Epoch:830 1/3...  Loss: 0.3163\n",
      "Epoch:831 1/3...  Loss: 0.1796\n",
      "Epoch:832 1/3...  Loss: 0.5478\n",
      "Epoch:833 1/3...  Loss: 0.2126\n",
      "Epoch:834 1/3...  Loss: 0.3033\n",
      "Epoch:835 1/3...  Loss: 0.2361\n",
      "Epoch:836 1/3...  Loss: 0.3919\n",
      "Epoch:837 1/3...  Loss: 0.4516\n",
      "Epoch:838 1/3...  Loss: 0.3078\n",
      "Epoch:839 1/3...  Loss: 0.1887\n",
      "Epoch:840 1/3...  Loss: 0.3690\n",
      "Epoch:841 1/3...  Loss: 0.3460\n",
      "Epoch:842 1/3...  Loss: 0.3104\n",
      "Epoch:843 1/3...  Loss: 0.2720\n",
      "Epoch:844 1/3...  Loss: 0.3908\n",
      "Epoch:845 1/3...  Loss: 0.3053\n",
      "Epoch:846 1/3...  Loss: 0.3392\n",
      "Epoch:847 1/3...  Loss: 0.1688\n",
      "Epoch:848 1/3...  Loss: 0.1757\n",
      "Epoch:849 1/3...  Loss: 0.4741\n",
      "Epoch:850 1/3...  Loss: 0.2554\n",
      "Epoch:851 1/3...  Loss: 0.2467\n",
      "Epoch:852 1/3...  Loss: 0.3100\n",
      "Epoch:853 1/3...  Loss: 0.3169\n",
      "Epoch:854 1/3...  Loss: 0.3436\n",
      "Epoch:855 1/3...  Loss: 0.3593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:856 1/3...  Loss: 0.2079\n",
      "Epoch:857 1/3...  Loss: 0.3014\n",
      "Epoch:858 1/3...  Loss: 0.4821\n",
      "Epoch:859 1/3...  Loss: 0.3020\n",
      "Epoch:860 1/3...  Loss: 0.2598\n",
      "Epoch:861 1/3...  Loss: 0.2058\n",
      "Epoch:862 1/3...  Loss: 0.3055\n",
      "Epoch:863 1/3...  Loss: 0.2573\n",
      "Epoch:864 1/3...  Loss: 0.2319\n",
      "Epoch:865 1/3...  Loss: 0.5341\n",
      "Epoch:866 1/3...  Loss: 0.2088\n",
      "Epoch:867 1/3...  Loss: 0.4750\n",
      "Epoch:868 1/3...  Loss: 0.1629\n",
      "Epoch:869 1/3...  Loss: 0.3877\n",
      "Epoch:870 1/3...  Loss: 0.2620\n",
      "Epoch:871 1/3...  Loss: 0.3068\n",
      "Epoch:872 1/3...  Loss: 0.4156\n",
      "Epoch:873 1/3...  Loss: 0.3536\n",
      "Epoch:874 1/3...  Loss: 0.2329\n",
      "Epoch:875 1/3...  Loss: 0.3352\n",
      "Epoch:876 1/3...  Loss: 0.3491\n",
      "Epoch:877 1/3...  Loss: 0.3683\n",
      "Epoch:878 1/3...  Loss: 0.5812\n",
      "Epoch:879 1/3...  Loss: 0.2613\n",
      "Epoch:880 1/3...  Loss: 0.2873\n",
      "Epoch:881 1/3...  Loss: 0.5143\n",
      "Epoch:882 1/3...  Loss: 0.4849\n",
      "Epoch:883 1/3...  Loss: 0.5875\n",
      "Epoch:884 1/3...  Loss: 0.2933\n",
      "Epoch:885 1/3...  Loss: 0.5229\n",
      "Epoch:886 1/3...  Loss: 0.3804\n",
      "Epoch:887 1/3...  Loss: 0.3935\n",
      "Epoch:888 1/3...  Loss: 0.3092\n",
      "Epoch:889 1/3...  Loss: 0.4303\n",
      "Epoch:890 1/3...  Loss: 0.2789\n",
      "Epoch:891 1/3...  Loss: 0.4535\n",
      "Epoch:892 1/3...  Loss: 0.2913\n",
      "Epoch:893 1/3...  Loss: 0.1333\n",
      "Epoch:894 1/3...  Loss: 0.2814\n",
      "Epoch:895 1/3...  Loss: 0.2497\n",
      "Epoch:896 1/3...  Loss: 0.3002\n",
      "Epoch:897 1/3...  Loss: 0.3544\n",
      "Epoch:898 1/3...  Loss: 0.3595\n",
      "Epoch:899 1/3...  Loss: 0.1952\n",
      "Epoch:900 1/3...  Loss: 0.3034\n",
      "Epoch:901 1/3...  Loss: 0.4611\n",
      "Epoch:902 1/3...  Loss: 0.2789\n",
      "Epoch:903 1/3...  Loss: 0.3208\n",
      "Epoch:904 1/3...  Loss: 0.5782\n",
      "Epoch:905 1/3...  Loss: 0.3281\n",
      "Epoch:906 1/3...  Loss: 0.3449\n",
      "Epoch:907 1/3...  Loss: 0.2160\n",
      "Epoch:908 1/3...  Loss: 0.2554\n",
      "Epoch:909 1/3...  Loss: 0.4331\n",
      "Epoch:910 1/3...  Loss: 0.1887\n",
      "Epoch:911 1/3...  Loss: 0.2772\n",
      "Epoch:912 1/3...  Loss: 0.2597\n",
      "Epoch:913 1/3...  Loss: 0.4465\n",
      "Epoch:914 1/3...  Loss: 0.4019\n",
      "Epoch:915 1/3...  Loss: 0.3063\n",
      "Epoch:916 1/3...  Loss: 0.2028\n",
      "Epoch:917 1/3...  Loss: 0.3412\n",
      "Epoch:918 1/3...  Loss: 0.6150\n",
      "Epoch:919 1/3...  Loss: 0.3017\n",
      "Epoch:920 1/3...  Loss: 0.2780\n",
      "Epoch:921 1/3...  Loss: 0.2020\n",
      "Epoch:922 1/3...  Loss: 0.4442\n",
      "Epoch:923 1/3...  Loss: 0.3976\n",
      "Epoch:924 1/3...  Loss: 0.2514\n",
      "Epoch:925 1/3...  Loss: 0.4724\n",
      "Epoch:926 1/3...  Loss: 0.2616\n",
      "Epoch:927 1/3...  Loss: 0.2640\n",
      "Epoch:928 1/3...  Loss: 0.1900\n",
      "Epoch:929 1/3...  Loss: 0.4388\n",
      "Epoch:930 1/3...  Loss: 0.5043\n",
      "Epoch:931 1/3...  Loss: 0.1810\n",
      "Epoch:932 1/3...  Loss: 0.3963\n",
      "Epoch:933 1/3...  Loss: 0.2213\n",
      "Epoch:934 1/3...  Loss: 0.1911\n",
      "Epoch:935 1/3...  Loss: 0.2224\n",
      "Epoch:936 1/3...  Loss: 0.2077\n",
      "Epoch:937 1/3...  Loss: 0.5456\n",
      "Epoch:938 1/3...  Loss: 0.1813\n",
      "Epoch:939 2/3...  Loss: 0.2187\n",
      "Epoch:940 2/3...  Loss: 0.2863\n",
      "Epoch:941 2/3...  Loss: 0.5489\n",
      "Epoch:942 2/3...  Loss: 0.2873\n",
      "Epoch:943 2/3...  Loss: 0.2646\n",
      "Epoch:944 2/3...  Loss: 0.3401\n",
      "Epoch:945 2/3...  Loss: 0.3491\n",
      "Epoch:946 2/3...  Loss: 0.3342\n",
      "Epoch:947 2/3...  Loss: 0.2281\n",
      "Epoch:948 2/3...  Loss: 0.2695\n",
      "Epoch:949 2/3...  Loss: 0.2935\n",
      "Epoch:950 2/3...  Loss: 0.1484\n",
      "Epoch:951 2/3...  Loss: 0.3032\n",
      "Epoch:952 2/3...  Loss: 0.3348\n",
      "Epoch:953 2/3...  Loss: 0.1974\n",
      "Epoch:954 2/3...  Loss: 0.3570\n",
      "Epoch:955 2/3...  Loss: 0.4306\n",
      "Epoch:956 2/3...  Loss: 0.5959\n",
      "Epoch:957 2/3...  Loss: 0.4576\n",
      "Epoch:958 2/3...  Loss: 0.5025\n",
      "Epoch:959 2/3...  Loss: 0.2383\n",
      "Epoch:960 2/3...  Loss: 0.2999\n",
      "Epoch:961 2/3...  Loss: 0.3789\n",
      "Epoch:962 2/3...  Loss: 0.3624\n",
      "Epoch:963 2/3...  Loss: 0.2111\n",
      "Epoch:964 2/3...  Loss: 0.2614\n",
      "Epoch:965 2/3...  Loss: 0.3520\n",
      "Epoch:966 2/3...  Loss: 0.2561\n",
      "Epoch:967 2/3...  Loss: 0.2397\n",
      "Epoch:968 2/3...  Loss: 0.3935\n",
      "Epoch:969 2/3...  Loss: 0.2510\n",
      "Epoch:970 2/3...  Loss: 0.6097\n",
      "Epoch:971 2/3...  Loss: 0.2419\n",
      "Epoch:972 2/3...  Loss: 0.4074\n",
      "Epoch:973 2/3...  Loss: 0.2202\n",
      "Epoch:974 2/3...  Loss: 0.4055\n",
      "Epoch:975 2/3...  Loss: 0.3616\n",
      "Epoch:976 2/3...  Loss: 0.1074\n",
      "Epoch:977 2/3...  Loss: 0.2900\n",
      "Epoch:978 2/3...  Loss: 0.3674\n",
      "Epoch:979 2/3...  Loss: 0.3873\n",
      "Epoch:980 2/3...  Loss: 0.2644\n",
      "Epoch:981 2/3...  Loss: 0.3120\n",
      "Epoch:982 2/3...  Loss: 0.3943\n",
      "Epoch:983 2/3...  Loss: 0.4084\n",
      "Epoch:984 2/3...  Loss: 0.3314\n",
      "Epoch:985 2/3...  Loss: 0.2365\n",
      "Epoch:986 2/3...  Loss: 0.3370\n",
      "Epoch:987 2/3...  Loss: 0.1529\n",
      "Epoch:988 2/3...  Loss: 0.3460\n",
      "Epoch:989 2/3...  Loss: 0.3452\n",
      "Epoch:990 2/3...  Loss: 0.2967\n",
      "Epoch:991 2/3...  Loss: 0.4338\n",
      "Epoch:992 2/3...  Loss: 0.3372\n",
      "Epoch:993 2/3...  Loss: 0.2230\n",
      "Epoch:994 2/3...  Loss: 0.3370\n",
      "Epoch:995 2/3...  Loss: 0.2695\n",
      "Epoch:996 2/3...  Loss: 0.3375\n",
      "Epoch:997 2/3...  Loss: 0.1369\n",
      "Epoch:998 2/3...  Loss: 0.3357\n",
      "Epoch:999 2/3...  Loss: 0.2236\n",
      "Epoch:1000 2/3...  Loss: 0.2931\n",
      "Epoch:1001 2/3...  Loss: 0.3154\n",
      "Epoch:1002 2/3...  Loss: 0.5253\n",
      "Epoch:1003 2/3...  Loss: 0.4995\n",
      "Epoch:1004 2/3...  Loss: 0.2823\n",
      "Epoch:1005 2/3...  Loss: 0.3059\n",
      "Epoch:1006 2/3...  Loss: 0.3802\n",
      "Epoch:1007 2/3...  Loss: 0.2399\n",
      "Epoch:1008 2/3...  Loss: 0.2837\n",
      "Epoch:1009 2/3...  Loss: 0.3192\n",
      "Epoch:1010 2/3...  Loss: 0.3787\n",
      "Epoch:1011 2/3...  Loss: 0.3307\n",
      "Epoch:1012 2/3...  Loss: 0.2794\n",
      "Epoch:1013 2/3...  Loss: 0.3899\n",
      "Epoch:1014 2/3...  Loss: 0.5659\n",
      "Epoch:1015 2/3...  Loss: 0.3892\n",
      "Epoch:1016 2/3...  Loss: 0.3542\n",
      "Epoch:1017 2/3...  Loss: 0.4612\n",
      "Epoch:1018 2/3...  Loss: 0.2673\n",
      "Epoch:1019 2/3...  Loss: 0.4324\n",
      "Epoch:1020 2/3...  Loss: 0.2369\n",
      "Epoch:1021 2/3...  Loss: 0.3153\n",
      "Epoch:1022 2/3...  Loss: 0.1997\n",
      "Epoch:1023 2/3...  Loss: 0.1812\n",
      "Epoch:1024 2/3...  Loss: 0.2125\n",
      "Epoch:1025 2/3...  Loss: 0.3728\n",
      "Epoch:1026 2/3...  Loss: 0.3431\n",
      "Epoch:1027 2/3...  Loss: 0.4644\n",
      "Epoch:1028 2/3...  Loss: 0.2315\n",
      "Epoch:1029 2/3...  Loss: 0.3536\n",
      "Epoch:1030 2/3...  Loss: 0.3754\n",
      "Epoch:1031 2/3...  Loss: 0.1964\n",
      "Epoch:1032 2/3...  Loss: 0.3384\n",
      "Epoch:1033 2/3...  Loss: 0.2308\n",
      "Epoch:1034 2/3...  Loss: 0.3445\n",
      "Epoch:1035 2/3...  Loss: 0.3161\n",
      "Epoch:1036 2/3...  Loss: 0.1912\n",
      "Epoch:1037 2/3...  Loss: 0.3281\n",
      "Epoch:1038 2/3...  Loss: 0.2002\n",
      "Epoch:1039 2/3...  Loss: 0.3613\n",
      "Epoch:1040 2/3...  Loss: 0.3550\n",
      "Epoch:1041 2/3...  Loss: 0.2401\n",
      "Epoch:1042 2/3...  Loss: 0.3181\n",
      "Epoch:1043 2/3...  Loss: 0.4055\n",
      "Epoch:1044 2/3...  Loss: 0.4903\n",
      "Epoch:1045 2/3...  Loss: 0.2445\n",
      "Epoch:1046 2/3...  Loss: 0.1688\n",
      "Epoch:1047 2/3...  Loss: 0.2567\n",
      "Epoch:1048 2/3...  Loss: 0.2147\n",
      "Epoch:1049 2/3...  Loss: 0.6159\n",
      "Epoch:1050 2/3...  Loss: 0.3470\n",
      "Epoch:1051 2/3...  Loss: 0.5447\n",
      "Epoch:1052 2/3...  Loss: 0.4096\n",
      "Epoch:1053 2/3...  Loss: 0.3729\n",
      "Epoch:1054 2/3...  Loss: 0.2696\n",
      "Epoch:1055 2/3...  Loss: 0.1726\n",
      "Epoch:1056 2/3...  Loss: 0.2058\n",
      "Epoch:1057 2/3...  Loss: 0.4023\n",
      "Epoch:1058 2/3...  Loss: 0.1978\n",
      "Epoch:1059 2/3...  Loss: 0.3399\n",
      "Epoch:1060 2/3...  Loss: 0.5890\n",
      "Epoch:1061 2/3...  Loss: 0.5868\n",
      "Epoch:1062 2/3...  Loss: 0.3341\n",
      "Epoch:1063 2/3...  Loss: 0.3489\n",
      "Epoch:1064 2/3...  Loss: 0.1981\n",
      "Epoch:1065 2/3...  Loss: 0.2917\n",
      "Epoch:1066 2/3...  Loss: 0.2715\n",
      "Epoch:1067 2/3...  Loss: 0.3539\n",
      "Epoch:1068 2/3...  Loss: 0.2607\n",
      "Epoch:1069 2/3...  Loss: 0.3881\n",
      "Epoch:1070 2/3...  Loss: 0.2508\n",
      "Epoch:1071 2/3...  Loss: 0.4831\n",
      "Epoch:1072 2/3...  Loss: 0.4349\n",
      "Epoch:1073 2/3...  Loss: 0.3538\n",
      "Epoch:1074 2/3...  Loss: 0.2709\n",
      "Epoch:1075 2/3...  Loss: 0.2480\n",
      "Epoch:1076 2/3...  Loss: 0.3103\n",
      "Epoch:1077 2/3...  Loss: 0.3574\n",
      "Epoch:1078 2/3...  Loss: 0.3351\n",
      "Epoch:1079 2/3...  Loss: 0.1664\n",
      "Epoch:1080 2/3...  Loss: 0.2272\n",
      "Epoch:1081 2/3...  Loss: 0.5284\n",
      "Epoch:1082 2/3...  Loss: 0.2384\n",
      "Epoch:1083 2/3...  Loss: 0.3818\n",
      "Epoch:1084 2/3...  Loss: 0.2938\n",
      "Epoch:1085 2/3...  Loss: 0.3657\n",
      "Epoch:1086 2/3...  Loss: 0.1846\n",
      "Epoch:1087 2/3...  Loss: 0.4089\n",
      "Epoch:1088 2/3...  Loss: 0.2228\n",
      "Epoch:1089 2/3...  Loss: 0.2410\n",
      "Epoch:1090 2/3...  Loss: 0.2992\n",
      "Epoch:1091 2/3...  Loss: 0.2879\n",
      "Epoch:1092 2/3...  Loss: 0.4135\n",
      "Epoch:1093 2/3...  Loss: 0.2123\n",
      "Epoch:1094 2/3...  Loss: 0.3953\n",
      "Epoch:1095 2/3...  Loss: 0.2324\n",
      "Epoch:1096 2/3...  Loss: 0.4482\n",
      "Epoch:1097 2/3...  Loss: 0.3588\n",
      "Epoch:1098 2/3...  Loss: 0.2266\n",
      "Epoch:1099 2/3...  Loss: 0.5754\n",
      "Epoch:1100 2/3...  Loss: 0.2437\n",
      "Epoch:1101 2/3...  Loss: 0.2235\n",
      "Epoch:1102 2/3...  Loss: 0.3923\n",
      "Epoch:1103 2/3...  Loss: 0.3959\n",
      "Epoch:1104 2/3...  Loss: 0.4313\n",
      "Epoch:1105 2/3...  Loss: 0.2458\n",
      "Epoch:1106 2/3...  Loss: 0.2529\n",
      "Epoch:1107 2/3...  Loss: 0.2183\n",
      "Epoch:1108 2/3...  Loss: 0.4317\n",
      "Epoch:1109 2/3...  Loss: 0.3554\n",
      "Epoch:1110 2/3...  Loss: 0.4460\n",
      "Epoch:1111 2/3...  Loss: 0.3422\n",
      "Epoch:1112 2/3...  Loss: 0.4194\n",
      "Epoch:1113 2/3...  Loss: 0.3414\n",
      "Epoch:1114 2/3...  Loss: 0.3042\n",
      "Epoch:1115 2/3...  Loss: 0.4409\n",
      "Epoch:1116 2/3...  Loss: 0.3542\n",
      "Epoch:1117 2/3...  Loss: 0.4002\n",
      "Epoch:1118 2/3...  Loss: 0.2924\n",
      "Epoch:1119 2/3...  Loss: 0.2649\n",
      "Epoch:1120 2/3...  Loss: 0.5564\n",
      "Epoch:1121 2/3...  Loss: 0.2930\n",
      "Epoch:1122 2/3...  Loss: 0.5121\n",
      "Epoch:1123 2/3...  Loss: 0.4661\n",
      "Epoch:1124 2/3...  Loss: 0.2656\n",
      "Epoch:1125 2/3...  Loss: 0.4606\n",
      "Epoch:1126 2/3...  Loss: 0.5473\n",
      "Epoch:1127 2/3...  Loss: 0.2997\n",
      "Epoch:1128 2/3...  Loss: 0.3406\n",
      "Epoch:1129 2/3...  Loss: 0.2160\n",
      "Epoch:1130 2/3...  Loss: 0.3104\n",
      "Epoch:1131 2/3...  Loss: 0.5225\n",
      "Epoch:1132 2/3...  Loss: 0.2751\n",
      "Epoch:1133 2/3...  Loss: 0.2758\n",
      "Epoch:1134 2/3...  Loss: 0.4223\n",
      "Epoch:1135 2/3...  Loss: 0.2536\n",
      "Epoch:1136 2/3...  Loss: 0.4908\n",
      "Epoch:1137 2/3...  Loss: 0.2843\n",
      "Epoch:1138 2/3...  Loss: 0.3540\n",
      "Epoch:1139 2/3...  Loss: 0.4712\n",
      "Epoch:1140 2/3...  Loss: 0.2357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1141 2/3...  Loss: 0.3371\n",
      "Epoch:1142 2/3...  Loss: 0.3352\n",
      "Epoch:1143 2/3...  Loss: 0.2942\n",
      "Epoch:1144 2/3...  Loss: 0.3762\n",
      "Epoch:1145 2/3...  Loss: 0.1994\n",
      "Epoch:1146 2/3...  Loss: 0.3254\n",
      "Epoch:1147 2/3...  Loss: 0.3778\n",
      "Epoch:1148 2/3...  Loss: 0.1535\n",
      "Epoch:1149 2/3...  Loss: 0.2585\n",
      "Epoch:1150 2/3...  Loss: 0.4456\n",
      "Epoch:1151 2/3...  Loss: 0.2817\n",
      "Epoch:1152 2/3...  Loss: 0.2352\n",
      "Epoch:1153 2/3...  Loss: 0.3655\n",
      "Epoch:1154 2/3...  Loss: 0.2421\n",
      "Epoch:1155 2/3...  Loss: 0.2424\n",
      "Epoch:1156 2/3...  Loss: 0.3169\n",
      "Epoch:1157 2/3...  Loss: 0.5751\n",
      "Epoch:1158 2/3...  Loss: 0.2680\n",
      "Epoch:1159 2/3...  Loss: 0.2749\n",
      "Epoch:1160 2/3...  Loss: 0.2173\n",
      "Epoch:1161 2/3...  Loss: 0.4216\n",
      "Epoch:1162 2/3...  Loss: 0.1941\n",
      "Epoch:1163 2/3...  Loss: 0.4404\n",
      "Epoch:1164 2/3...  Loss: 0.5176\n",
      "Epoch:1165 2/3...  Loss: 0.3085\n",
      "Epoch:1166 2/3...  Loss: 0.2313\n",
      "Epoch:1167 2/3...  Loss: 0.2269\n",
      "Epoch:1168 2/3...  Loss: 0.2897\n",
      "Epoch:1169 2/3...  Loss: 0.2630\n",
      "Epoch:1170 2/3...  Loss: 0.3843\n",
      "Epoch:1171 2/3...  Loss: 0.2594\n",
      "Epoch:1172 2/3...  Loss: 0.3537\n",
      "Epoch:1173 2/3...  Loss: 0.3333\n",
      "Epoch:1174 2/3...  Loss: 0.3475\n",
      "Epoch:1175 2/3...  Loss: 0.2768\n",
      "Epoch:1176 2/3...  Loss: 0.4104\n",
      "Epoch:1177 2/3...  Loss: 0.2703\n",
      "Epoch:1178 2/3...  Loss: 0.3324\n",
      "Epoch:1179 2/3...  Loss: 0.2727\n",
      "Epoch:1180 2/3...  Loss: 0.3839\n",
      "Epoch:1181 2/3...  Loss: 0.3154\n",
      "Epoch:1182 2/3...  Loss: 0.4928\n",
      "Epoch:1183 2/3...  Loss: 0.4731\n",
      "Epoch:1184 2/3...  Loss: 0.4043\n",
      "Epoch:1185 2/3...  Loss: 0.1553\n",
      "Epoch:1186 2/3...  Loss: 0.1805\n",
      "Epoch:1187 2/3...  Loss: 0.3328\n",
      "Epoch:1188 2/3...  Loss: 0.4417\n",
      "Epoch:1189 2/3...  Loss: 0.3255\n",
      "Epoch:1190 2/3...  Loss: 0.2995\n",
      "Epoch:1191 2/3...  Loss: 0.4214\n",
      "Epoch:1192 2/3...  Loss: 0.3957\n",
      "Epoch:1193 2/3...  Loss: 0.3968\n",
      "Epoch:1194 2/3...  Loss: 0.1871\n",
      "Epoch:1195 2/3...  Loss: 0.3700\n",
      "Epoch:1196 2/3...  Loss: 0.4470\n",
      "Epoch:1197 2/3...  Loss: 0.2813\n",
      "Epoch:1198 2/3...  Loss: 0.1579\n",
      "Epoch:1199 2/3...  Loss: 0.4110\n",
      "Epoch:1200 2/3...  Loss: 0.5079\n",
      "Epoch:1201 2/3...  Loss: 0.3819\n",
      "Epoch:1202 2/3...  Loss: 0.1764\n",
      "Epoch:1203 2/3...  Loss: 0.3782\n",
      "Epoch:1204 2/3...  Loss: 0.1939\n",
      "Epoch:1205 2/3...  Loss: 0.5160\n",
      "Epoch:1206 2/3...  Loss: 0.5779\n",
      "Epoch:1207 2/3...  Loss: 0.3788\n",
      "Epoch:1208 2/3...  Loss: 0.3009\n",
      "Epoch:1209 2/3...  Loss: 0.2219\n",
      "Epoch:1210 2/3...  Loss: 0.3486\n",
      "Epoch:1211 2/3...  Loss: 0.2507\n",
      "Epoch:1212 2/3...  Loss: 0.3002\n",
      "Epoch:1213 2/3...  Loss: 0.4450\n",
      "Epoch:1214 2/3...  Loss: 0.4257\n",
      "Epoch:1215 2/3...  Loss: 0.5245\n",
      "Epoch:1216 2/3...  Loss: 0.2288\n",
      "Epoch:1217 2/3...  Loss: 0.1629\n",
      "Epoch:1218 2/3...  Loss: 0.4377\n",
      "Epoch:1219 2/3...  Loss: 0.3860\n",
      "Epoch:1220 2/3...  Loss: 0.5158\n",
      "Epoch:1221 2/3...  Loss: 0.1838\n",
      "Epoch:1222 2/3...  Loss: 0.2912\n",
      "Epoch:1223 2/3...  Loss: 0.3424\n",
      "Epoch:1224 2/3...  Loss: 0.3565\n",
      "Epoch:1225 2/3...  Loss: 0.6489\n",
      "Epoch:1226 2/3...  Loss: 0.2977\n",
      "Epoch:1227 2/3...  Loss: 0.2236\n",
      "Epoch:1228 2/3...  Loss: 0.4263\n",
      "Epoch:1229 2/3...  Loss: 0.2378\n",
      "Epoch:1230 2/3...  Loss: 0.4750\n",
      "Epoch:1231 2/3...  Loss: 0.4198\n",
      "Epoch:1232 2/3...  Loss: 0.2313\n",
      "Epoch:1233 2/3...  Loss: 0.2052\n",
      "Epoch:1234 2/3...  Loss: 0.4660\n",
      "Epoch:1235 2/3...  Loss: 0.3794\n",
      "Epoch:1236 2/3...  Loss: 0.2952\n",
      "Epoch:1237 2/3...  Loss: 0.2823\n",
      "Epoch:1238 2/3...  Loss: 0.2771\n",
      "Epoch:1239 2/3...  Loss: 0.2020\n",
      "Epoch:1240 2/3...  Loss: 0.4041\n",
      "Epoch:1241 2/3...  Loss: 0.2498\n",
      "Epoch:1242 2/3...  Loss: 0.2806\n",
      "Epoch:1243 2/3...  Loss: 0.3118\n",
      "Epoch:1244 2/3...  Loss: 0.2328\n",
      "Epoch:1245 2/3...  Loss: 0.4466\n",
      "Epoch:1246 2/3...  Loss: 0.3330\n",
      "Epoch:1247 2/3...  Loss: 0.3033\n",
      "Epoch:1248 2/3...  Loss: 0.4373\n",
      "Epoch:1249 2/3...  Loss: 0.3765\n",
      "Epoch:1250 2/3...  Loss: 0.2606\n",
      "Epoch:1251 2/3...  Loss: 0.2890\n",
      "Epoch:1252 2/3...  Loss: 0.2250\n",
      "Epoch:1253 2/3...  Loss: 0.3690\n",
      "Epoch:1254 2/3...  Loss: 0.2425\n",
      "Epoch:1255 2/3...  Loss: 0.2597\n",
      "Epoch:1256 2/3...  Loss: 0.1951\n",
      "Epoch:1257 2/3...  Loss: 0.2739\n",
      "Epoch:1258 2/3...  Loss: 0.3241\n",
      "Epoch:1259 2/3...  Loss: 0.2485\n",
      "Epoch:1260 2/3...  Loss: 0.3162\n",
      "Epoch:1261 2/3...  Loss: 0.2583\n",
      "Epoch:1262 2/3...  Loss: 0.3099\n",
      "Epoch:1263 2/3...  Loss: 0.3195\n",
      "Epoch:1264 2/3...  Loss: 0.2822\n",
      "Epoch:1265 2/3...  Loss: 0.1671\n",
      "Epoch:1266 2/3...  Loss: 0.3924\n",
      "Epoch:1267 2/3...  Loss: 0.5045\n",
      "Epoch:1268 2/3...  Loss: 0.3489\n",
      "Epoch:1269 2/3...  Loss: 0.2519\n",
      "Epoch:1270 2/3...  Loss: 0.3969\n",
      "Epoch:1271 2/3...  Loss: 0.2622\n",
      "Epoch:1272 2/3...  Loss: 0.4612\n",
      "Epoch:1273 2/3...  Loss: 0.2444\n",
      "Epoch:1274 2/3...  Loss: 0.2323\n",
      "Epoch:1275 2/3...  Loss: 0.2229\n",
      "Epoch:1276 2/3...  Loss: 0.3092\n",
      "Epoch:1277 2/3...  Loss: 0.3116\n",
      "Epoch:1278 2/3...  Loss: 0.2580\n",
      "Epoch:1279 2/3...  Loss: 0.1636\n",
      "Epoch:1280 2/3...  Loss: 0.3883\n",
      "Epoch:1281 2/3...  Loss: 0.4594\n",
      "Epoch:1282 2/3...  Loss: 0.4009\n",
      "Epoch:1283 2/3...  Loss: 0.2135\n",
      "Epoch:1284 2/3...  Loss: 0.2328\n",
      "Epoch:1285 2/3...  Loss: 0.2873\n",
      "Epoch:1286 2/3...  Loss: 0.3443\n",
      "Epoch:1287 2/3...  Loss: 0.2954\n",
      "Epoch:1288 2/3...  Loss: 0.3894\n",
      "Epoch:1289 2/3...  Loss: 0.1992\n",
      "Epoch:1290 2/3...  Loss: 0.3348\n",
      "Epoch:1291 2/3...  Loss: 0.4432\n",
      "Epoch:1292 2/3...  Loss: 0.3443\n",
      "Epoch:1293 2/3...  Loss: 0.2131\n",
      "Epoch:1294 2/3...  Loss: 0.2005\n",
      "Epoch:1295 2/3...  Loss: 0.2626\n",
      "Epoch:1296 2/3...  Loss: 0.3590\n",
      "Epoch:1297 2/3...  Loss: 0.4213\n",
      "Epoch:1298 2/3...  Loss: 0.1653\n",
      "Epoch:1299 2/3...  Loss: 0.3340\n",
      "Epoch:1300 2/3...  Loss: 0.3168\n",
      "Epoch:1301 2/3...  Loss: 0.2988\n",
      "Epoch:1302 2/3...  Loss: 0.4130\n",
      "Epoch:1303 2/3...  Loss: 0.2400\n",
      "Epoch:1304 2/3...  Loss: 0.4857\n",
      "Epoch:1305 2/3...  Loss: 0.3264\n",
      "Epoch:1306 2/3...  Loss: 0.2517\n",
      "Epoch:1307 2/3...  Loss: 0.1144\n",
      "Epoch:1308 2/3...  Loss: 0.1815\n",
      "Epoch:1309 2/3...  Loss: 0.2837\n",
      "Epoch:1310 2/3...  Loss: 0.2657\n",
      "Epoch:1311 2/3...  Loss: 0.4973\n",
      "Epoch:1312 2/3...  Loss: 0.3252\n",
      "Epoch:1313 2/3...  Loss: 0.3787\n",
      "Epoch:1314 2/3...  Loss: 0.2751\n",
      "Epoch:1315 2/3...  Loss: 0.3270\n",
      "Epoch:1316 2/3...  Loss: 0.2662\n",
      "Epoch:1317 2/3...  Loss: 0.3989\n",
      "Epoch:1318 2/3...  Loss: 0.4719\n",
      "Epoch:1319 2/3...  Loss: 0.1738\n",
      "Epoch:1320 2/3...  Loss: 0.4406\n",
      "Epoch:1321 2/3...  Loss: 0.3079\n",
      "Epoch:1322 2/3...  Loss: 0.2087\n",
      "Epoch:1323 2/3...  Loss: 0.3679\n",
      "Epoch:1324 2/3...  Loss: 0.2991\n",
      "Epoch:1325 2/3...  Loss: 0.4185\n",
      "Epoch:1326 2/3...  Loss: 0.1574\n",
      "Epoch:1327 2/3...  Loss: 0.4211\n",
      "Epoch:1328 2/3...  Loss: 0.2967\n",
      "Epoch:1329 2/3...  Loss: 0.2679\n",
      "Epoch:1330 2/3...  Loss: 0.3137\n",
      "Epoch:1331 2/3...  Loss: 0.3476\n",
      "Epoch:1332 2/3...  Loss: 0.3743\n",
      "Epoch:1333 2/3...  Loss: 0.2302\n",
      "Epoch:1334 2/3...  Loss: 0.3711\n",
      "Epoch:1335 2/3...  Loss: 0.3364\n",
      "Epoch:1336 2/3...  Loss: 0.3763\n",
      "Epoch:1337 2/3...  Loss: 0.4085\n",
      "Epoch:1338 2/3...  Loss: 0.2965\n",
      "Epoch:1339 2/3...  Loss: 0.2820\n",
      "Epoch:1340 2/3...  Loss: 0.3804\n",
      "Epoch:1341 2/3...  Loss: 0.4340\n",
      "Epoch:1342 2/3...  Loss: 0.3625\n",
      "Epoch:1343 2/3...  Loss: 0.2516\n",
      "Epoch:1344 2/3...  Loss: 0.3455\n",
      "Epoch:1345 2/3...  Loss: 0.2728\n",
      "Epoch:1346 2/3...  Loss: 0.3746\n",
      "Epoch:1347 2/3...  Loss: 0.3881\n",
      "Epoch:1348 2/3...  Loss: 0.2077\n",
      "Epoch:1349 2/3...  Loss: 0.4442\n",
      "Epoch:1350 2/3...  Loss: 0.2763\n",
      "Epoch:1351 2/3...  Loss: 0.5946\n",
      "Epoch:1352 2/3...  Loss: 0.3666\n",
      "Epoch:1353 2/3...  Loss: 0.3114\n",
      "Epoch:1354 2/3...  Loss: 0.3796\n",
      "Epoch:1355 2/3...  Loss: 0.3552\n",
      "Epoch:1356 2/3...  Loss: 0.4278\n",
      "Epoch:1357 2/3...  Loss: 0.1644\n",
      "Epoch:1358 2/3...  Loss: 0.3029\n",
      "Epoch:1359 2/3...  Loss: 0.3790\n",
      "Epoch:1360 2/3...  Loss: 0.2914\n",
      "Epoch:1361 2/3...  Loss: 0.3409\n",
      "Epoch:1362 2/3...  Loss: 0.3601\n",
      "Epoch:1363 2/3...  Loss: 0.3752\n",
      "Epoch:1364 2/3...  Loss: 0.4484\n",
      "Epoch:1365 2/3...  Loss: 0.4552\n",
      "Epoch:1366 2/3...  Loss: 0.3834\n",
      "Epoch:1367 2/3...  Loss: 0.2593\n",
      "Epoch:1368 2/3...  Loss: 0.1976\n",
      "Epoch:1369 2/3...  Loss: 0.7723\n",
      "Epoch:1370 2/3...  Loss: 0.4776\n",
      "Epoch:1371 2/3...  Loss: 0.1130\n",
      "Epoch:1372 2/3...  Loss: 0.2241\n",
      "Epoch:1373 2/3...  Loss: 0.3017\n",
      "Epoch:1374 2/3...  Loss: 0.3089\n",
      "Epoch:1375 2/3...  Loss: 0.2244\n",
      "Epoch:1376 2/3...  Loss: 0.3353\n",
      "Epoch:1377 2/3...  Loss: 0.2658\n",
      "Epoch:1378 2/3...  Loss: 0.4575\n",
      "Epoch:1379 2/3...  Loss: 0.4769\n",
      "Epoch:1380 2/3...  Loss: 0.4188\n",
      "Epoch:1381 2/3...  Loss: 0.3524\n",
      "Epoch:1382 2/3...  Loss: 0.4174\n",
      "Epoch:1383 2/3...  Loss: 0.3412\n",
      "Epoch:1384 2/3...  Loss: 0.2642\n",
      "Epoch:1385 2/3...  Loss: 0.1552\n",
      "Epoch:1386 2/3...  Loss: 0.2929\n",
      "Epoch:1387 2/3...  Loss: 0.2721\n",
      "Epoch:1388 2/3...  Loss: 0.2779\n",
      "Epoch:1389 2/3...  Loss: 0.2895\n",
      "Epoch:1390 2/3...  Loss: 0.2485\n",
      "Epoch:1391 2/3...  Loss: 0.3799\n",
      "Epoch:1392 2/3...  Loss: 0.4474\n",
      "Epoch:1393 2/3...  Loss: 0.3522\n",
      "Epoch:1394 2/3...  Loss: 0.3435\n",
      "Epoch:1395 2/3...  Loss: 0.2761\n",
      "Epoch:1396 2/3...  Loss: 0.3013\n",
      "Epoch:1397 2/3...  Loss: 0.3327\n",
      "Epoch:1398 2/3...  Loss: 0.4595\n",
      "Epoch:1399 2/3...  Loss: 0.3068\n",
      "Epoch:1400 2/3...  Loss: 0.4522\n",
      "Epoch:1401 2/3...  Loss: 0.2285\n",
      "Epoch:1402 2/3...  Loss: 0.2926\n",
      "Epoch:1403 2/3...  Loss: 0.2751\n",
      "Epoch:1404 2/3...  Loss: 0.2138\n",
      "Epoch:1405 2/3...  Loss: 0.4701\n",
      "Epoch:1406 2/3...  Loss: 0.1543\n",
      "Epoch:1407 2/3...  Loss: 0.3112\n",
      "Epoch:1408 2/3...  Loss: 0.1978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1409 2/3...  Loss: 0.4110\n",
      "Epoch:1410 2/3...  Loss: 0.1283\n",
      "Epoch:1411 2/3...  Loss: 0.2701\n",
      "Epoch:1412 2/3...  Loss: 0.3001\n",
      "Epoch:1413 2/3...  Loss: 0.4159\n",
      "Epoch:1414 2/3...  Loss: 0.2025\n",
      "Epoch:1415 2/3...  Loss: 0.3743\n",
      "Epoch:1416 2/3...  Loss: 0.4095\n",
      "Epoch:1417 2/3...  Loss: 0.2124\n",
      "Epoch:1418 2/3...  Loss: 0.1924\n",
      "Epoch:1419 2/3...  Loss: 0.6384\n",
      "Epoch:1420 2/3...  Loss: 0.2578\n",
      "Epoch:1421 2/3...  Loss: 0.2814\n",
      "Epoch:1422 2/3...  Loss: 0.1902\n",
      "Epoch:1423 2/3...  Loss: 0.4143\n",
      "Epoch:1424 2/3...  Loss: 0.1951\n",
      "Epoch:1425 2/3...  Loss: 0.3925\n",
      "Epoch:1426 2/3...  Loss: 0.3235\n",
      "Epoch:1427 2/3...  Loss: 0.3449\n",
      "Epoch:1428 2/3...  Loss: 0.5420\n",
      "Epoch:1429 2/3...  Loss: 0.4269\n",
      "Epoch:1430 2/3...  Loss: 0.5008\n",
      "Epoch:1431 2/3...  Loss: 0.2683\n",
      "Epoch:1432 2/3...  Loss: 0.2868\n",
      "Epoch:1433 2/3...  Loss: 0.2934\n",
      "Epoch:1434 2/3...  Loss: 0.2193\n",
      "Epoch:1435 2/3...  Loss: 0.2122\n",
      "Epoch:1436 2/3...  Loss: 0.3580\n",
      "Epoch:1437 2/3...  Loss: 0.2858\n",
      "Epoch:1438 2/3...  Loss: 0.2149\n",
      "Epoch:1439 2/3...  Loss: 0.4617\n",
      "Epoch:1440 2/3...  Loss: 0.1839\n",
      "Epoch:1441 2/3...  Loss: 0.3850\n",
      "Epoch:1442 2/3...  Loss: 0.2756\n",
      "Epoch:1443 2/3...  Loss: 0.2454\n",
      "Epoch:1444 2/3...  Loss: 0.3604\n",
      "Epoch:1445 2/3...  Loss: 0.2516\n",
      "Epoch:1446 2/3...  Loss: 0.3240\n",
      "Epoch:1447 2/3...  Loss: 0.2999\n",
      "Epoch:1448 2/3...  Loss: 0.4768\n",
      "Epoch:1449 2/3...  Loss: 0.4032\n",
      "Epoch:1450 2/3...  Loss: 0.3274\n",
      "Epoch:1451 2/3...  Loss: 0.3072\n",
      "Epoch:1452 2/3...  Loss: 0.3449\n",
      "Epoch:1453 2/3...  Loss: 0.4261\n",
      "Epoch:1454 2/3...  Loss: 0.1984\n",
      "Epoch:1455 2/3...  Loss: 0.4259\n",
      "Epoch:1456 2/3...  Loss: 0.3198\n",
      "Epoch:1457 2/3...  Loss: 0.2304\n",
      "Epoch:1458 2/3...  Loss: 0.1371\n",
      "Epoch:1459 2/3...  Loss: 0.5356\n",
      "Epoch:1460 2/3...  Loss: 0.2550\n",
      "Epoch:1461 2/3...  Loss: 0.3237\n",
      "Epoch:1462 2/3...  Loss: 0.3748\n",
      "Epoch:1463 2/3...  Loss: 0.3830\n",
      "Epoch:1464 2/3...  Loss: 0.3529\n",
      "Epoch:1465 2/3...  Loss: 0.2287\n",
      "Epoch:1466 2/3...  Loss: 0.2199\n",
      "Epoch:1467 2/3...  Loss: 0.3670\n",
      "Epoch:1468 2/3...  Loss: 0.3249\n",
      "Epoch:1469 2/3...  Loss: 0.2717\n",
      "Epoch:1470 2/3...  Loss: 0.2978\n",
      "Epoch:1471 2/3...  Loss: 0.2595\n",
      "Epoch:1472 2/3...  Loss: 0.5493\n",
      "Epoch:1473 2/3...  Loss: 0.3220\n",
      "Epoch:1474 2/3...  Loss: 0.4144\n",
      "Epoch:1475 2/3...  Loss: 0.2927\n",
      "Epoch:1476 2/3...  Loss: 0.3283\n",
      "Epoch:1477 2/3...  Loss: 0.2142\n",
      "Epoch:1478 2/3...  Loss: 0.3440\n",
      "Epoch:1479 2/3...  Loss: 0.3329\n",
      "Epoch:1480 2/3...  Loss: 0.3637\n",
      "Epoch:1481 2/3...  Loss: 0.2137\n",
      "Epoch:1482 2/3...  Loss: 0.3624\n",
      "Epoch:1483 2/3...  Loss: 0.2513\n",
      "Epoch:1484 2/3...  Loss: 0.3600\n",
      "Epoch:1485 2/3...  Loss: 0.2591\n",
      "Epoch:1486 2/3...  Loss: 0.3330\n",
      "Epoch:1487 2/3...  Loss: 0.1740\n",
      "Epoch:1488 2/3...  Loss: 0.2480\n",
      "Epoch:1489 2/3...  Loss: 0.3289\n",
      "Epoch:1490 2/3...  Loss: 0.3512\n",
      "Epoch:1491 2/3...  Loss: 0.2785\n",
      "Epoch:1492 2/3...  Loss: 0.3597\n",
      "Epoch:1493 2/3...  Loss: 0.2179\n",
      "Epoch:1494 2/3...  Loss: 0.4078\n",
      "Epoch:1495 2/3...  Loss: 0.2796\n",
      "Epoch:1496 2/3...  Loss: 0.2643\n",
      "Epoch:1497 2/3...  Loss: 0.2199\n",
      "Epoch:1498 2/3...  Loss: 0.7371\n",
      "Epoch:1499 2/3...  Loss: 0.2354\n",
      "Epoch:1500 2/3...  Loss: 0.2294\n",
      "Epoch:1501 2/3...  Loss: 0.3578\n",
      "Epoch:1502 2/3...  Loss: 0.1873\n",
      "Epoch:1503 2/3...  Loss: 0.3430\n",
      "Epoch:1504 2/3...  Loss: 0.1979\n",
      "Epoch:1505 2/3...  Loss: 0.3074\n",
      "Epoch:1506 2/3...  Loss: 0.3952\n",
      "Epoch:1507 2/3...  Loss: 0.2947\n",
      "Epoch:1508 2/3...  Loss: 0.3145\n",
      "Epoch:1509 2/3...  Loss: 0.2869\n",
      "Epoch:1510 2/3...  Loss: 0.2253\n",
      "Epoch:1511 2/3...  Loss: 0.3012\n",
      "Epoch:1512 2/3...  Loss: 0.2123\n",
      "Epoch:1513 2/3...  Loss: 0.4651\n",
      "Epoch:1514 2/3...  Loss: 0.4316\n",
      "Epoch:1515 2/3...  Loss: 0.5258\n",
      "Epoch:1516 2/3...  Loss: 0.3021\n",
      "Epoch:1517 2/3...  Loss: 0.1886\n",
      "Epoch:1518 2/3...  Loss: 0.3670\n",
      "Epoch:1519 2/3...  Loss: 0.3139\n",
      "Epoch:1520 2/3...  Loss: 0.1885\n",
      "Epoch:1521 2/3...  Loss: 0.1945\n",
      "Epoch:1522 2/3...  Loss: 0.3713\n",
      "Epoch:1523 2/3...  Loss: 0.2553\n",
      "Epoch:1524 2/3...  Loss: 0.4518\n",
      "Epoch:1525 2/3...  Loss: 0.3083\n",
      "Epoch:1526 2/3...  Loss: 0.2697\n",
      "Epoch:1527 2/3...  Loss: 0.3968\n",
      "Epoch:1528 2/3...  Loss: 0.2514\n",
      "Epoch:1529 2/3...  Loss: 0.2800\n",
      "Epoch:1530 2/3...  Loss: 0.1711\n",
      "Epoch:1531 2/3...  Loss: 0.2855\n",
      "Epoch:1532 2/3...  Loss: 0.3467\n",
      "Epoch:1533 2/3...  Loss: 0.3035\n",
      "Epoch:1534 2/3...  Loss: 0.4970\n",
      "Epoch:1535 2/3...  Loss: 0.1835\n",
      "Epoch:1536 2/3...  Loss: 0.2543\n",
      "Epoch:1537 2/3...  Loss: 0.3009\n",
      "Epoch:1538 2/3...  Loss: 0.4915\n",
      "Epoch:1539 2/3...  Loss: 0.3440\n",
      "Epoch:1540 2/3...  Loss: 0.3443\n",
      "Epoch:1541 2/3...  Loss: 0.4874\n",
      "Epoch:1542 2/3...  Loss: 0.4039\n",
      "Epoch:1543 2/3...  Loss: 0.2637\n",
      "Epoch:1544 2/3...  Loss: 0.3170\n",
      "Epoch:1545 2/3...  Loss: 0.2990\n",
      "Epoch:1546 2/3...  Loss: 0.3596\n",
      "Epoch:1547 2/3...  Loss: 0.3537\n",
      "Epoch:1548 2/3...  Loss: 0.1309\n",
      "Epoch:1549 2/3...  Loss: 0.3981\n",
      "Epoch:1550 2/3...  Loss: 0.3964\n",
      "Epoch:1551 2/3...  Loss: 0.2318\n",
      "Epoch:1552 2/3...  Loss: 0.3981\n",
      "Epoch:1553 2/3...  Loss: 0.2724\n",
      "Epoch:1554 2/3...  Loss: 0.1794\n",
      "Epoch:1555 2/3...  Loss: 0.1783\n",
      "Epoch:1556 2/3...  Loss: 0.3529\n",
      "Epoch:1557 2/3...  Loss: 0.3024\n",
      "Epoch:1558 2/3...  Loss: 0.3248\n",
      "Epoch:1559 2/3...  Loss: 0.2633\n",
      "Epoch:1560 2/3...  Loss: 0.2884\n",
      "Epoch:1561 2/3...  Loss: 0.2189\n",
      "Epoch:1562 2/3...  Loss: 0.2798\n",
      "Epoch:1563 2/3...  Loss: 0.4220\n",
      "Epoch:1564 2/3...  Loss: 0.1716\n",
      "Epoch:1565 2/3...  Loss: 0.2658\n",
      "Epoch:1566 2/3...  Loss: 0.3583\n",
      "Epoch:1567 2/3...  Loss: 0.2950\n",
      "Epoch:1568 2/3...  Loss: 0.5360\n",
      "Epoch:1569 2/3...  Loss: 0.1622\n",
      "Epoch:1570 2/3...  Loss: 0.2452\n",
      "Epoch:1571 2/3...  Loss: 0.2840\n",
      "Epoch:1572 2/3...  Loss: 0.4553\n",
      "Epoch:1573 2/3...  Loss: 0.1876\n",
      "Epoch:1574 2/3...  Loss: 0.3480\n",
      "Epoch:1575 2/3...  Loss: 0.2875\n",
      "Epoch:1576 2/3...  Loss: 0.3922\n",
      "Epoch:1577 2/3...  Loss: 0.1858\n",
      "Epoch:1578 2/3...  Loss: 0.4165\n",
      "Epoch:1579 2/3...  Loss: 0.3788\n",
      "Epoch:1580 2/3...  Loss: 0.1890\n",
      "Epoch:1581 2/3...  Loss: 0.2395\n",
      "Epoch:1582 2/3...  Loss: 0.2006\n",
      "Epoch:1583 2/3...  Loss: 0.2068\n",
      "Epoch:1584 2/3...  Loss: 0.2971\n",
      "Epoch:1585 2/3...  Loss: 0.3051\n",
      "Epoch:1586 2/3...  Loss: 0.3466\n",
      "Epoch:1587 2/3...  Loss: 0.2069\n",
      "Epoch:1588 2/3...  Loss: 0.3314\n",
      "Epoch:1589 2/3...  Loss: 0.3065\n",
      "Epoch:1590 2/3...  Loss: 0.3230\n",
      "Epoch:1591 2/3...  Loss: 0.4706\n",
      "Epoch:1592 2/3...  Loss: 0.3578\n",
      "Epoch:1593 2/3...  Loss: 0.3861\n",
      "Epoch:1594 2/3...  Loss: 0.3206\n",
      "Epoch:1595 2/3...  Loss: 0.4016\n",
      "Epoch:1596 2/3...  Loss: 0.2585\n",
      "Epoch:1597 2/3...  Loss: 0.4357\n",
      "Epoch:1598 2/3...  Loss: 0.3079\n",
      "Epoch:1599 2/3...  Loss: 0.2059\n",
      "Epoch:1600 2/3...  Loss: 0.1814\n",
      "Epoch:1601 2/3...  Loss: 0.1259\n",
      "Epoch:1602 2/3...  Loss: 0.2584\n",
      "Epoch:1603 2/3...  Loss: 0.3416\n",
      "Epoch:1604 2/3...  Loss: 0.4448\n",
      "Epoch:1605 2/3...  Loss: 0.2300\n",
      "Epoch:1606 2/3...  Loss: 0.2919\n",
      "Epoch:1607 2/3...  Loss: 0.1754\n",
      "Epoch:1608 2/3...  Loss: 0.2003\n",
      "Epoch:1609 2/3...  Loss: 0.3527\n",
      "Epoch:1610 2/3...  Loss: 0.3592\n",
      "Epoch:1611 2/3...  Loss: 0.4396\n",
      "Epoch:1612 2/3...  Loss: 0.5272\n",
      "Epoch:1613 2/3...  Loss: 0.3111\n",
      "Epoch:1614 2/3...  Loss: 0.3202\n",
      "Epoch:1615 2/3...  Loss: 0.2444\n",
      "Epoch:1616 2/3...  Loss: 0.3313\n",
      "Epoch:1617 2/3...  Loss: 0.3216\n",
      "Epoch:1618 2/3...  Loss: 0.4394\n",
      "Epoch:1619 2/3...  Loss: 0.3683\n",
      "Epoch:1620 2/3...  Loss: 0.4634\n",
      "Epoch:1621 2/3...  Loss: 0.4117\n",
      "Epoch:1622 2/3...  Loss: 0.2583\n",
      "Epoch:1623 2/3...  Loss: 0.4352\n",
      "Epoch:1624 2/3...  Loss: 0.2732\n",
      "Epoch:1625 2/3...  Loss: 0.4615\n",
      "Epoch:1626 2/3...  Loss: 0.2571\n",
      "Epoch:1627 2/3...  Loss: 0.3058\n",
      "Epoch:1628 2/3...  Loss: 0.4459\n",
      "Epoch:1629 2/3...  Loss: 0.2028\n",
      "Epoch:1630 2/3...  Loss: 0.4771\n",
      "Epoch:1631 2/3...  Loss: 0.3634\n",
      "Epoch:1632 2/3...  Loss: 0.4379\n",
      "Epoch:1633 2/3...  Loss: 0.2550\n",
      "Epoch:1634 2/3...  Loss: 0.3143\n",
      "Epoch:1635 2/3...  Loss: 0.3122\n",
      "Epoch:1636 2/3...  Loss: 0.3039\n",
      "Epoch:1637 2/3...  Loss: 0.2177\n",
      "Epoch:1638 2/3...  Loss: 0.4566\n",
      "Epoch:1639 2/3...  Loss: 0.1552\n",
      "Epoch:1640 2/3...  Loss: 0.3193\n",
      "Epoch:1641 2/3...  Loss: 0.4016\n",
      "Epoch:1642 2/3...  Loss: 0.3166\n",
      "Epoch:1643 2/3...  Loss: 0.3551\n",
      "Epoch:1644 2/3...  Loss: 0.4416\n",
      "Epoch:1645 2/3...  Loss: 0.2576\n",
      "Epoch:1646 2/3...  Loss: 0.2779\n",
      "Epoch:1647 2/3...  Loss: 0.1180\n",
      "Epoch:1648 2/3...  Loss: 0.3713\n",
      "Epoch:1649 2/3...  Loss: 0.2828\n",
      "Epoch:1650 2/3...  Loss: 0.4184\n",
      "Epoch:1651 2/3...  Loss: 0.6015\n",
      "Epoch:1652 2/3...  Loss: 0.3412\n",
      "Epoch:1653 2/3...  Loss: 0.3330\n",
      "Epoch:1654 2/3...  Loss: 0.2780\n",
      "Epoch:1655 2/3...  Loss: 0.4678\n",
      "Epoch:1656 2/3...  Loss: 0.1943\n",
      "Epoch:1657 2/3...  Loss: 0.2651\n",
      "Epoch:1658 2/3...  Loss: 0.3772\n",
      "Epoch:1659 2/3...  Loss: 0.2611\n",
      "Epoch:1660 2/3...  Loss: 0.4154\n",
      "Epoch:1661 2/3...  Loss: 0.2652\n",
      "Epoch:1662 2/3...  Loss: 0.1758\n",
      "Epoch:1663 2/3...  Loss: 0.2621\n",
      "Epoch:1664 2/3...  Loss: 0.5484\n",
      "Epoch:1665 2/3...  Loss: 0.4464\n",
      "Epoch:1666 2/3...  Loss: 0.3265\n",
      "Epoch:1667 2/3...  Loss: 0.2390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1668 2/3...  Loss: 0.4113\n",
      "Epoch:1669 2/3...  Loss: 0.3299\n",
      "Epoch:1670 2/3...  Loss: 0.3495\n",
      "Epoch:1671 2/3...  Loss: 0.3083\n",
      "Epoch:1672 2/3...  Loss: 0.3349\n",
      "Epoch:1673 2/3...  Loss: 0.3403\n",
      "Epoch:1674 2/3...  Loss: 0.2581\n",
      "Epoch:1675 2/3...  Loss: 0.2844\n",
      "Epoch:1676 2/3...  Loss: 0.2977\n",
      "Epoch:1677 2/3...  Loss: 0.3863\n",
      "Epoch:1678 2/3...  Loss: 0.3781\n",
      "Epoch:1679 2/3...  Loss: 0.5654\n",
      "Epoch:1680 2/3...  Loss: 0.2135\n",
      "Epoch:1681 2/3...  Loss: 0.2209\n",
      "Epoch:1682 2/3...  Loss: 0.2330\n",
      "Epoch:1683 2/3...  Loss: 0.2225\n",
      "Epoch:1684 2/3...  Loss: 0.3694\n",
      "Epoch:1685 2/3...  Loss: 0.3267\n",
      "Epoch:1686 2/3...  Loss: 0.2235\n",
      "Epoch:1687 2/3...  Loss: 0.6804\n",
      "Epoch:1688 2/3...  Loss: 0.2630\n",
      "Epoch:1689 2/3...  Loss: 0.2762\n",
      "Epoch:1690 2/3...  Loss: 0.5175\n",
      "Epoch:1691 2/3...  Loss: 0.2815\n",
      "Epoch:1692 2/3...  Loss: 0.3341\n",
      "Epoch:1693 2/3...  Loss: 0.1561\n",
      "Epoch:1694 2/3...  Loss: 0.2575\n",
      "Epoch:1695 2/3...  Loss: 0.3767\n",
      "Epoch:1696 2/3...  Loss: 0.5154\n",
      "Epoch:1697 2/3...  Loss: 0.2429\n",
      "Epoch:1698 2/3...  Loss: 0.3141\n",
      "Epoch:1699 2/3...  Loss: 0.3982\n",
      "Epoch:1700 2/3...  Loss: 0.4052\n",
      "Epoch:1701 2/3...  Loss: 0.3029\n",
      "Epoch:1702 2/3...  Loss: 0.3060\n",
      "Epoch:1703 2/3...  Loss: 0.2274\n",
      "Epoch:1704 2/3...  Loss: 0.2727\n",
      "Epoch:1705 2/3...  Loss: 0.3940\n",
      "Epoch:1706 2/3...  Loss: 0.4147\n",
      "Epoch:1707 2/3...  Loss: 0.2853\n",
      "Epoch:1708 2/3...  Loss: 0.3146\n",
      "Epoch:1709 2/3...  Loss: 0.3221\n",
      "Epoch:1710 2/3...  Loss: 0.2787\n",
      "Epoch:1711 2/3...  Loss: 0.2965\n",
      "Epoch:1712 2/3...  Loss: 0.3499\n",
      "Epoch:1713 2/3...  Loss: 0.2459\n",
      "Epoch:1714 2/3...  Loss: 0.4444\n",
      "Epoch:1715 2/3...  Loss: 0.5296\n",
      "Epoch:1716 2/3...  Loss: 0.4226\n",
      "Epoch:1717 2/3...  Loss: 0.3829\n",
      "Epoch:1718 2/3...  Loss: 0.3857\n",
      "Epoch:1719 2/3...  Loss: 0.3435\n",
      "Epoch:1720 2/3...  Loss: 0.4849\n",
      "Epoch:1721 2/3...  Loss: 0.1608\n",
      "Epoch:1722 2/3...  Loss: 0.2139\n",
      "Epoch:1723 2/3...  Loss: 0.2962\n",
      "Epoch:1724 2/3...  Loss: 0.1958\n",
      "Epoch:1725 2/3...  Loss: 0.1917\n",
      "Epoch:1726 2/3...  Loss: 0.3478\n",
      "Epoch:1727 2/3...  Loss: 0.3059\n",
      "Epoch:1728 2/3...  Loss: 0.3532\n",
      "Epoch:1729 2/3...  Loss: 0.3421\n",
      "Epoch:1730 2/3...  Loss: 0.4209\n",
      "Epoch:1731 2/3...  Loss: 0.3028\n",
      "Epoch:1732 2/3...  Loss: 0.3923\n",
      "Epoch:1733 2/3...  Loss: 0.3359\n",
      "Epoch:1734 2/3...  Loss: 0.4418\n",
      "Epoch:1735 2/3...  Loss: 0.4280\n",
      "Epoch:1736 2/3...  Loss: 0.6019\n",
      "Epoch:1737 2/3...  Loss: 0.3598\n",
      "Epoch:1738 2/3...  Loss: 0.3291\n",
      "Epoch:1739 2/3...  Loss: 0.4965\n",
      "Epoch:1740 2/3...  Loss: 0.4282\n",
      "Epoch:1741 2/3...  Loss: 0.2318\n",
      "Epoch:1742 2/3...  Loss: 0.3339\n",
      "Epoch:1743 2/3...  Loss: 0.2449\n",
      "Epoch:1744 2/3...  Loss: 0.2229\n",
      "Epoch:1745 2/3...  Loss: 0.2532\n",
      "Epoch:1746 2/3...  Loss: 0.2040\n",
      "Epoch:1747 2/3...  Loss: 0.4222\n",
      "Epoch:1748 2/3...  Loss: 0.2622\n",
      "Epoch:1749 2/3...  Loss: 0.4541\n",
      "Epoch:1750 2/3...  Loss: 0.3968\n",
      "Epoch:1751 2/3...  Loss: 0.3922\n",
      "Epoch:1752 2/3...  Loss: 0.5506\n",
      "Epoch:1753 2/3...  Loss: 0.4420\n",
      "Epoch:1754 2/3...  Loss: 0.3253\n",
      "Epoch:1755 2/3...  Loss: 0.2095\n",
      "Epoch:1756 2/3...  Loss: 0.3220\n",
      "Epoch:1757 2/3...  Loss: 0.4852\n",
      "Epoch:1758 2/3...  Loss: 0.1980\n",
      "Epoch:1759 2/3...  Loss: 0.3531\n",
      "Epoch:1760 2/3...  Loss: 0.4125\n",
      "Epoch:1761 2/3...  Loss: 0.3962\n",
      "Epoch:1762 2/3...  Loss: 0.4622\n",
      "Epoch:1763 2/3...  Loss: 0.4388\n",
      "Epoch:1764 2/3...  Loss: 0.3956\n",
      "Epoch:1765 2/3...  Loss: 0.2437\n",
      "Epoch:1766 2/3...  Loss: 0.3449\n",
      "Epoch:1767 2/3...  Loss: 0.2312\n",
      "Epoch:1768 2/3...  Loss: 0.2799\n",
      "Epoch:1769 2/3...  Loss: 0.3559\n",
      "Epoch:1770 2/3...  Loss: 0.3781\n",
      "Epoch:1771 2/3...  Loss: 0.3376\n",
      "Epoch:1772 2/3...  Loss: 0.2395\n",
      "Epoch:1773 2/3...  Loss: 0.3770\n",
      "Epoch:1774 2/3...  Loss: 0.4965\n",
      "Epoch:1775 2/3...  Loss: 0.2425\n",
      "Epoch:1776 2/3...  Loss: 0.2351\n",
      "Epoch:1777 2/3...  Loss: 0.2161\n",
      "Epoch:1778 2/3...  Loss: 0.3920\n",
      "Epoch:1779 2/3...  Loss: 0.2382\n",
      "Epoch:1780 2/3...  Loss: 0.2939\n",
      "Epoch:1781 2/3...  Loss: 0.2721\n",
      "Epoch:1782 2/3...  Loss: 0.3620\n",
      "Epoch:1783 2/3...  Loss: 0.1752\n",
      "Epoch:1784 2/3...  Loss: 0.3978\n",
      "Epoch:1785 2/3...  Loss: 0.2773\n",
      "Epoch:1786 2/3...  Loss: 0.1646\n",
      "Epoch:1787 2/3...  Loss: 0.4503\n",
      "Epoch:1788 2/3...  Loss: 0.4081\n",
      "Epoch:1789 2/3...  Loss: 0.2422\n",
      "Epoch:1790 2/3...  Loss: 0.4805\n",
      "Epoch:1791 2/3...  Loss: 0.2093\n",
      "Epoch:1792 2/3...  Loss: 0.2788\n",
      "Epoch:1793 2/3...  Loss: 0.3895\n",
      "Epoch:1794 2/3...  Loss: 0.3428\n",
      "Epoch:1795 2/3...  Loss: 0.4916\n",
      "Epoch:1796 2/3...  Loss: 0.6241\n",
      "Epoch:1797 2/3...  Loss: 0.2142\n",
      "Epoch:1798 2/3...  Loss: 0.3611\n",
      "Epoch:1799 2/3...  Loss: 0.2129\n",
      "Epoch:1800 2/3...  Loss: 0.2422\n",
      "Epoch:1801 2/3...  Loss: 0.4314\n",
      "Epoch:1802 2/3...  Loss: 0.1531\n",
      "Epoch:1803 2/3...  Loss: 0.2452\n",
      "Epoch:1804 2/3...  Loss: 0.2679\n",
      "Epoch:1805 2/3...  Loss: 0.3686\n",
      "Epoch:1806 2/3...  Loss: 0.2600\n",
      "Epoch:1807 2/3...  Loss: 0.1697\n",
      "Epoch:1808 2/3...  Loss: 0.3233\n",
      "Epoch:1809 2/3...  Loss: 0.2558\n",
      "Epoch:1810 2/3...  Loss: 0.2417\n",
      "Epoch:1811 2/3...  Loss: 0.5105\n",
      "Epoch:1812 2/3...  Loss: 0.3940\n",
      "Epoch:1813 2/3...  Loss: 0.3358\n",
      "Epoch:1814 2/3...  Loss: 0.3043\n",
      "Epoch:1815 2/3...  Loss: 0.1537\n",
      "Epoch:1816 2/3...  Loss: 0.2090\n",
      "Epoch:1817 2/3...  Loss: 0.2952\n",
      "Epoch:1818 2/3...  Loss: 0.2216\n",
      "Epoch:1819 2/3...  Loss: 0.3462\n",
      "Epoch:1820 2/3...  Loss: 0.4118\n",
      "Epoch:1821 2/3...  Loss: 0.4063\n",
      "Epoch:1822 2/3...  Loss: 0.2754\n",
      "Epoch:1823 2/3...  Loss: 0.2523\n",
      "Epoch:1824 2/3...  Loss: 0.4818\n",
      "Epoch:1825 2/3...  Loss: 0.3293\n",
      "Epoch:1826 2/3...  Loss: 0.2814\n",
      "Epoch:1827 2/3...  Loss: 0.4377\n",
      "Epoch:1828 2/3...  Loss: 0.2149\n",
      "Epoch:1829 2/3...  Loss: 0.3469\n",
      "Epoch:1830 2/3...  Loss: 0.1714\n",
      "Epoch:1831 2/3...  Loss: 0.2535\n",
      "Epoch:1832 2/3...  Loss: 0.2890\n",
      "Epoch:1833 2/3...  Loss: 0.3393\n",
      "Epoch:1834 2/3...  Loss: 0.3501\n",
      "Epoch:1835 2/3...  Loss: 0.2072\n",
      "Epoch:1836 2/3...  Loss: 0.3104\n",
      "Epoch:1837 2/3...  Loss: 0.2356\n",
      "Epoch:1838 2/3...  Loss: 0.3288\n",
      "Epoch:1839 2/3...  Loss: 0.2961\n",
      "Epoch:1840 2/3...  Loss: 0.4648\n",
      "Epoch:1841 2/3...  Loss: 0.4195\n",
      "Epoch:1842 2/3...  Loss: 0.3428\n",
      "Epoch:1843 2/3...  Loss: 0.2099\n",
      "Epoch:1844 2/3...  Loss: 0.3298\n",
      "Epoch:1845 2/3...  Loss: 0.3944\n",
      "Epoch:1846 2/3...  Loss: 0.3934\n",
      "Epoch:1847 2/3...  Loss: 0.2088\n",
      "Epoch:1848 2/3...  Loss: 0.2503\n",
      "Epoch:1849 2/3...  Loss: 0.2258\n",
      "Epoch:1850 2/3...  Loss: 0.3890\n",
      "Epoch:1851 2/3...  Loss: 0.3210\n",
      "Epoch:1852 2/3...  Loss: 0.1504\n",
      "Epoch:1853 2/3...  Loss: 0.1722\n",
      "Epoch:1854 2/3...  Loss: 0.5487\n",
      "Epoch:1855 2/3...  Loss: 0.2561\n",
      "Epoch:1856 2/3...  Loss: 0.4659\n",
      "Epoch:1857 2/3...  Loss: 0.3446\n",
      "Epoch:1858 2/3...  Loss: 0.2993\n",
      "Epoch:1859 2/3...  Loss: 0.3099\n",
      "Epoch:1860 2/3...  Loss: 0.4115\n",
      "Epoch:1861 2/3...  Loss: 0.3835\n",
      "Epoch:1862 2/3...  Loss: 0.2974\n",
      "Epoch:1863 2/3...  Loss: 0.1634\n",
      "Epoch:1864 2/3...  Loss: 0.2233\n",
      "Epoch:1865 2/3...  Loss: 0.3491\n",
      "Epoch:1866 2/3...  Loss: 0.2422\n",
      "Epoch:1867 2/3...  Loss: 0.3292\n",
      "Epoch:1868 2/3...  Loss: 0.2860\n",
      "Epoch:1869 2/3...  Loss: 0.3304\n",
      "Epoch:1870 2/3...  Loss: 0.2196\n",
      "Epoch:1871 2/3...  Loss: 0.4023\n",
      "Epoch:1872 2/3...  Loss: 0.2429\n",
      "Epoch:1873 2/3...  Loss: 0.1783\n",
      "Epoch:1874 2/3...  Loss: 0.1669\n",
      "Epoch:1875 2/3...  Loss: 0.2938\n",
      "Epoch:1876 2/3...  Loss: 0.4676\n",
      "Epoch:1877 3/3...  Loss: 0.3209\n",
      "Epoch:1878 3/3...  Loss: 0.1524\n",
      "Epoch:1879 3/3...  Loss: 0.2007\n",
      "Epoch:1880 3/3...  Loss: 0.2507\n",
      "Epoch:1881 3/3...  Loss: 0.6236\n",
      "Epoch:1882 3/3...  Loss: 0.2870\n",
      "Epoch:1883 3/3...  Loss: 0.4122\n",
      "Epoch:1884 3/3...  Loss: 0.2311\n",
      "Epoch:1885 3/3...  Loss: 0.4351\n",
      "Epoch:1886 3/3...  Loss: 0.3355\n",
      "Epoch:1887 3/3...  Loss: 0.2112\n",
      "Epoch:1888 3/3...  Loss: 0.5915\n",
      "Epoch:1889 3/3...  Loss: 0.3182\n",
      "Epoch:1890 3/3...  Loss: 0.2582\n",
      "Epoch:1891 3/3...  Loss: 0.3050\n",
      "Epoch:1892 3/3...  Loss: 0.3284\n",
      "Epoch:1893 3/3...  Loss: 0.2699\n",
      "Epoch:1894 3/3...  Loss: 0.2177\n",
      "Epoch:1895 3/3...  Loss: 0.2657\n",
      "Epoch:1896 3/3...  Loss: 0.2968\n",
      "Epoch:1897 3/3...  Loss: 0.1765\n",
      "Epoch:1898 3/3...  Loss: 0.3772\n",
      "Epoch:1899 3/3...  Loss: 0.4837\n",
      "Epoch:1900 3/3...  Loss: 0.2973\n",
      "Epoch:1901 3/3...  Loss: 0.3469\n",
      "Epoch:1902 3/3...  Loss: 0.2342\n",
      "Epoch:1903 3/3...  Loss: 0.3397\n",
      "Epoch:1904 3/3...  Loss: 0.3160\n",
      "Epoch:1905 3/3...  Loss: 0.3486\n",
      "Epoch:1906 3/3...  Loss: 0.3559\n",
      "Epoch:1907 3/3...  Loss: 0.2073\n",
      "Epoch:1908 3/3...  Loss: 0.1503\n",
      "Epoch:1909 3/3...  Loss: 0.3936\n",
      "Epoch:1910 3/3...  Loss: 0.2798\n",
      "Epoch:1911 3/3...  Loss: 0.2208\n",
      "Epoch:1912 3/3...  Loss: 0.2731\n",
      "Epoch:1913 3/3...  Loss: 0.5053\n",
      "Epoch:1914 3/3...  Loss: 0.2059\n",
      "Epoch:1915 3/3...  Loss: 0.4716\n",
      "Epoch:1916 3/3...  Loss: 0.1028\n",
      "Epoch:1917 3/3...  Loss: 0.4032\n",
      "Epoch:1918 3/3...  Loss: 0.2480\n",
      "Epoch:1919 3/3...  Loss: 0.4905\n",
      "Epoch:1920 3/3...  Loss: 0.2055\n",
      "Epoch:1921 3/3...  Loss: 0.1792\n",
      "Epoch:1922 3/3...  Loss: 0.2228\n",
      "Epoch:1923 3/3...  Loss: 0.2760\n",
      "Epoch:1924 3/3...  Loss: 0.5217\n",
      "Epoch:1925 3/3...  Loss: 0.3042\n",
      "Epoch:1926 3/3...  Loss: 0.2733\n",
      "Epoch:1927 3/3...  Loss: 0.3613\n",
      "Epoch:1928 3/3...  Loss: 0.2208\n",
      "Epoch:1929 3/3...  Loss: 0.3065\n",
      "Epoch:1930 3/3...  Loss: 0.2586\n",
      "Epoch:1931 3/3...  Loss: 0.2239\n",
      "Epoch:1932 3/3...  Loss: 0.2088\n",
      "Epoch:1933 3/3...  Loss: 0.3775\n",
      "Epoch:1934 3/3...  Loss: 0.2522\n",
      "Epoch:1935 3/3...  Loss: 0.4345\n",
      "Epoch:1936 3/3...  Loss: 0.2606\n",
      "Epoch:1937 3/3...  Loss: 0.2322\n",
      "Epoch:1938 3/3...  Loss: 0.2644\n",
      "Epoch:1939 3/3...  Loss: 0.3969\n",
      "Epoch:1940 3/3...  Loss: 0.2413\n",
      "Epoch:1941 3/3...  Loss: 0.3200\n",
      "Epoch:1942 3/3...  Loss: 0.3430\n",
      "Epoch:1943 3/3...  Loss: 0.4409\n",
      "Epoch:1944 3/3...  Loss: 0.1369\n",
      "Epoch:1945 3/3...  Loss: 0.1363\n",
      "Epoch:1946 3/3...  Loss: 0.3238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1947 3/3...  Loss: 0.3290\n",
      "Epoch:1948 3/3...  Loss: 0.2753\n",
      "Epoch:1949 3/3...  Loss: 0.3199\n",
      "Epoch:1950 3/3...  Loss: 0.3871\n",
      "Epoch:1951 3/3...  Loss: 0.3339\n",
      "Epoch:1952 3/3...  Loss: 0.3352\n",
      "Epoch:1953 3/3...  Loss: 0.2280\n",
      "Epoch:1954 3/3...  Loss: 0.2379\n",
      "Epoch:1955 3/3...  Loss: 0.3608\n",
      "Epoch:1956 3/3...  Loss: 0.2522\n",
      "Epoch:1957 3/3...  Loss: 0.3514\n",
      "Epoch:1958 3/3...  Loss: 0.1493\n",
      "Epoch:1959 3/3...  Loss: 0.2977\n",
      "Epoch:1960 3/3...  Loss: 0.3015\n",
      "Epoch:1961 3/3...  Loss: 0.3149\n",
      "Epoch:1962 3/3...  Loss: 0.1977\n",
      "Epoch:1963 3/3...  Loss: 0.2951\n",
      "Epoch:1964 3/3...  Loss: 0.2750\n",
      "Epoch:1965 3/3...  Loss: 0.2986\n",
      "Epoch:1966 3/3...  Loss: 0.2535\n",
      "Epoch:1967 3/3...  Loss: 0.2843\n",
      "Epoch:1968 3/3...  Loss: 0.2906\n",
      "Epoch:1969 3/3...  Loss: 0.2273\n",
      "Epoch:1970 3/3...  Loss: 0.2816\n",
      "Epoch:1971 3/3...  Loss: 0.3143\n",
      "Epoch:1972 3/3...  Loss: 0.3821\n",
      "Epoch:1973 3/3...  Loss: 0.4187\n",
      "Epoch:1974 3/3...  Loss: 0.5327\n",
      "Epoch:1975 3/3...  Loss: 0.3739\n",
      "Epoch:1976 3/3...  Loss: 0.2915\n",
      "Epoch:1977 3/3...  Loss: 0.3079\n",
      "Epoch:1978 3/3...  Loss: 0.3951\n",
      "Epoch:1979 3/3...  Loss: 0.2960\n",
      "Epoch:1980 3/3...  Loss: 0.3707\n",
      "Epoch:1981 3/3...  Loss: 0.2095\n",
      "Epoch:1982 3/3...  Loss: 0.5038\n",
      "Epoch:1983 3/3...  Loss: 0.2755\n",
      "Epoch:1984 3/3...  Loss: 0.3140\n",
      "Epoch:1985 3/3...  Loss: 0.3259\n",
      "Epoch:1986 3/3...  Loss: 0.2729\n",
      "Epoch:1987 3/3...  Loss: 0.2300\n",
      "Epoch:1988 3/3...  Loss: 0.3192\n",
      "Epoch:1989 3/3...  Loss: 0.4792\n",
      "Epoch:1990 3/3...  Loss: 0.3287\n",
      "Epoch:1991 3/3...  Loss: 0.3122\n",
      "Epoch:1992 3/3...  Loss: 0.2648\n",
      "Epoch:1993 3/3...  Loss: 0.1305\n",
      "Epoch:1994 3/3...  Loss: 0.5382\n",
      "Epoch:1995 3/3...  Loss: 0.2481\n",
      "Epoch:1996 3/3...  Loss: 0.3701\n",
      "Epoch:1997 3/3...  Loss: 0.3377\n",
      "Epoch:1998 3/3...  Loss: 0.1469\n",
      "Epoch:1999 3/3...  Loss: 0.3226\n",
      "Epoch:2000 3/3...  Loss: 0.2766\n",
      "Epoch:2001 3/3...  Loss: 0.4686\n",
      "Epoch:2002 3/3...  Loss: 0.3794\n",
      "Epoch:2003 3/3...  Loss: 0.2390\n",
      "Epoch:2004 3/3...  Loss: 0.2910\n",
      "Epoch:2005 3/3...  Loss: 0.3988\n",
      "Epoch:2006 3/3...  Loss: 0.2138\n",
      "Epoch:2007 3/3...  Loss: 0.3050\n",
      "Epoch:2008 3/3...  Loss: 0.3350\n",
      "Epoch:2009 3/3...  Loss: 0.2299\n",
      "Epoch:2010 3/3...  Loss: 0.2594\n",
      "Epoch:2011 3/3...  Loss: 0.3353\n",
      "Epoch:2012 3/3...  Loss: 0.4038\n",
      "Epoch:2013 3/3...  Loss: 0.2043\n",
      "Epoch:2014 3/3...  Loss: 0.3422\n",
      "Epoch:2015 3/3...  Loss: 0.3575\n",
      "Epoch:2016 3/3...  Loss: 0.2832\n",
      "Epoch:2017 3/3...  Loss: 0.5343\n",
      "Epoch:2018 3/3...  Loss: 0.4949\n",
      "Epoch:2019 3/3...  Loss: 0.2788\n",
      "Epoch:2020 3/3...  Loss: 0.5548\n",
      "Epoch:2021 3/3...  Loss: 0.2229\n",
      "Epoch:2022 3/3...  Loss: 0.3380\n",
      "Epoch:2023 3/3...  Loss: 0.2599\n",
      "Epoch:2024 3/3...  Loss: 0.2796\n",
      "Epoch:2025 3/3...  Loss: 0.1996\n",
      "Epoch:2026 3/3...  Loss: 0.4188\n",
      "Epoch:2027 3/3...  Loss: 0.2208\n",
      "Epoch:2028 3/3...  Loss: 0.3129\n",
      "Epoch:2029 3/3...  Loss: 0.3183\n",
      "Epoch:2030 3/3...  Loss: 0.3069\n",
      "Epoch:2031 3/3...  Loss: 0.3716\n",
      "Epoch:2032 3/3...  Loss: 0.3356\n",
      "Epoch:2033 3/3...  Loss: 0.4145\n",
      "Epoch:2034 3/3...  Loss: 0.5275\n",
      "Epoch:2035 3/3...  Loss: 0.2340\n",
      "Epoch:2036 3/3...  Loss: 0.1798\n",
      "Epoch:2037 3/3...  Loss: 0.2075\n",
      "Epoch:2038 3/3...  Loss: 0.3451\n",
      "Epoch:2039 3/3...  Loss: 0.4237\n",
      "Epoch:2040 3/3...  Loss: 0.3187\n",
      "Epoch:2041 3/3...  Loss: 0.2305\n",
      "Epoch:2042 3/3...  Loss: 0.4134\n",
      "Epoch:2043 3/3...  Loss: 0.2423\n",
      "Epoch:2044 3/3...  Loss: 0.2962\n",
      "Epoch:2045 3/3...  Loss: 0.2759\n",
      "Epoch:2046 3/3...  Loss: 0.1973\n",
      "Epoch:2047 3/3...  Loss: 0.1888\n",
      "Epoch:2048 3/3...  Loss: 0.4114\n",
      "Epoch:2049 3/3...  Loss: 0.2393\n",
      "Epoch:2050 3/3...  Loss: 0.4573\n",
      "Epoch:2051 3/3...  Loss: 0.1029\n",
      "Epoch:2052 3/3...  Loss: 0.2774\n",
      "Epoch:2053 3/3...  Loss: 0.3105\n",
      "Epoch:2054 3/3...  Loss: 0.4555\n",
      "Epoch:2055 3/3...  Loss: 0.3372\n",
      "Epoch:2056 3/3...  Loss: 0.3603\n",
      "Epoch:2057 3/3...  Loss: 0.1900\n",
      "Epoch:2058 3/3...  Loss: 0.1661\n",
      "Epoch:2059 3/3...  Loss: 0.3159\n",
      "Epoch:2060 3/3...  Loss: 0.3537\n",
      "Epoch:2061 3/3...  Loss: 0.3074\n",
      "Epoch:2062 3/3...  Loss: 0.2734\n",
      "Epoch:2063 3/3...  Loss: 0.2861\n",
      "Epoch:2064 3/3...  Loss: 0.3804\n",
      "Epoch:2065 3/3...  Loss: 0.2843\n",
      "Epoch:2066 3/3...  Loss: 0.2760\n",
      "Epoch:2067 3/3...  Loss: 0.2852\n",
      "Epoch:2068 3/3...  Loss: 0.3562\n",
      "Epoch:2069 3/3...  Loss: 0.2099\n",
      "Epoch:2070 3/3...  Loss: 0.3265\n",
      "Epoch:2071 3/3...  Loss: 0.3049\n",
      "Epoch:2072 3/3...  Loss: 0.3781\n",
      "Epoch:2073 3/3...  Loss: 0.3340\n",
      "Epoch:2074 3/3...  Loss: 0.1798\n",
      "Epoch:2075 3/3...  Loss: 0.1566\n",
      "Epoch:2076 3/3...  Loss: 0.3373\n",
      "Epoch:2077 3/3...  Loss: 0.2000\n",
      "Epoch:2078 3/3...  Loss: 0.2681\n",
      "Epoch:2079 3/3...  Loss: 0.4991\n",
      "Epoch:2080 3/3...  Loss: 0.3224\n",
      "Epoch:2081 3/3...  Loss: 0.2009\n",
      "Epoch:2082 3/3...  Loss: 0.2739\n",
      "Epoch:2083 3/3...  Loss: 0.3656\n",
      "Epoch:2084 3/3...  Loss: 0.2283\n",
      "Epoch:2085 3/3...  Loss: 0.2152\n",
      "Epoch:2086 3/3...  Loss: 0.2930\n",
      "Epoch:2087 3/3...  Loss: 0.3137\n",
      "Epoch:2088 3/3...  Loss: 0.2191\n",
      "Epoch:2089 3/3...  Loss: 0.5146\n",
      "Epoch:2090 3/3...  Loss: 0.1356\n",
      "Epoch:2091 3/3...  Loss: 0.2239\n",
      "Epoch:2092 3/3...  Loss: 0.1299\n",
      "Epoch:2093 3/3...  Loss: 0.2598\n",
      "Epoch:2094 3/3...  Loss: 0.2498\n",
      "Epoch:2095 3/3...  Loss: 0.4716\n",
      "Epoch:2096 3/3...  Loss: 0.4203\n",
      "Epoch:2097 3/3...  Loss: 0.1806\n",
      "Epoch:2098 3/3...  Loss: 0.3191\n",
      "Epoch:2099 3/3...  Loss: 0.2628\n",
      "Epoch:2100 3/3...  Loss: 0.2931\n",
      "Epoch:2101 3/3...  Loss: 0.3720\n",
      "Epoch:2102 3/3...  Loss: 0.3289\n",
      "Epoch:2103 3/3...  Loss: 0.3667\n",
      "Epoch:2104 3/3...  Loss: 0.2296\n",
      "Epoch:2105 3/3...  Loss: 0.2381\n",
      "Epoch:2106 3/3...  Loss: 0.2436\n",
      "Epoch:2107 3/3...  Loss: 0.2250\n",
      "Epoch:2108 3/3...  Loss: 0.3537\n",
      "Epoch:2109 3/3...  Loss: 0.1666\n",
      "Epoch:2110 3/3...  Loss: 0.3440\n",
      "Epoch:2111 3/3...  Loss: 0.2433\n",
      "Epoch:2112 3/3...  Loss: 0.4152\n",
      "Epoch:2113 3/3...  Loss: 0.3430\n",
      "Epoch:2114 3/3...  Loss: 0.2108\n",
      "Epoch:2115 3/3...  Loss: 0.3792\n",
      "Epoch:2116 3/3...  Loss: 0.2904\n",
      "Epoch:2117 3/3...  Loss: 0.2307\n",
      "Epoch:2118 3/3...  Loss: 0.2889\n",
      "Epoch:2119 3/3...  Loss: 0.3960\n",
      "Epoch:2120 3/3...  Loss: 0.3443\n",
      "Epoch:2121 3/3...  Loss: 0.1898\n",
      "Epoch:2122 3/3...  Loss: 0.3109\n",
      "Epoch:2123 3/3...  Loss: 0.3877\n",
      "Epoch:2124 3/3...  Loss: 0.4396\n",
      "Epoch:2125 3/3...  Loss: 0.3144\n",
      "Epoch:2126 3/3...  Loss: 0.2453\n",
      "Epoch:2127 3/3...  Loss: 0.2239\n",
      "Epoch:2128 3/3...  Loss: 0.2172\n",
      "Epoch:2129 3/3...  Loss: 0.4144\n",
      "Epoch:2130 3/3...  Loss: 0.1648\n",
      "Epoch:2131 3/3...  Loss: 0.1657\n",
      "Epoch:2132 3/3...  Loss: 0.4654\n",
      "Epoch:2133 3/3...  Loss: 0.4224\n",
      "Epoch:2134 3/3...  Loss: 0.1883\n",
      "Epoch:2135 3/3...  Loss: 0.1892\n",
      "Epoch:2136 3/3...  Loss: 0.2680\n",
      "Epoch:2137 3/3...  Loss: 0.4748\n",
      "Epoch:2138 3/3...  Loss: 0.4404\n",
      "Epoch:2139 3/3...  Loss: 0.2599\n",
      "Epoch:2140 3/3...  Loss: 0.2910\n",
      "Epoch:2141 3/3...  Loss: 0.2376\n",
      "Epoch:2142 3/3...  Loss: 0.4628\n",
      "Epoch:2143 3/3...  Loss: 0.2514\n",
      "Epoch:2144 3/3...  Loss: 0.3496\n",
      "Epoch:2145 3/3...  Loss: 0.3238\n",
      "Epoch:2146 3/3...  Loss: 0.3377\n",
      "Epoch:2147 3/3...  Loss: 0.2640\n",
      "Epoch:2148 3/3...  Loss: 0.2518\n",
      "Epoch:2149 3/3...  Loss: 0.2109\n",
      "Epoch:2150 3/3...  Loss: 0.1352\n",
      "Epoch:2151 3/3...  Loss: 0.2645\n",
      "Epoch:2152 3/3...  Loss: 0.1369\n",
      "Epoch:2153 3/3...  Loss: 0.6012\n",
      "Epoch:2154 3/3...  Loss: 0.3852\n",
      "Epoch:2155 3/3...  Loss: 0.5420\n",
      "Epoch:2156 3/3...  Loss: 0.2406\n",
      "Epoch:2157 3/3...  Loss: 0.2385\n",
      "Epoch:2158 3/3...  Loss: 0.6297\n",
      "Epoch:2159 3/3...  Loss: 0.3381\n",
      "Epoch:2160 3/3...  Loss: 0.4794\n",
      "Epoch:2161 3/3...  Loss: 0.2832\n",
      "Epoch:2162 3/3...  Loss: 0.1869\n",
      "Epoch:2163 3/3...  Loss: 0.3368\n",
      "Epoch:2164 3/3...  Loss: 0.2345\n",
      "Epoch:2165 3/3...  Loss: 0.2250\n",
      "Epoch:2166 3/3...  Loss: 0.3213\n",
      "Epoch:2167 3/3...  Loss: 0.2170\n",
      "Epoch:2168 3/3...  Loss: 0.1683\n",
      "Epoch:2169 3/3...  Loss: 0.3678\n",
      "Epoch:2170 3/3...  Loss: 0.4389\n",
      "Epoch:2171 3/3...  Loss: 0.4655\n",
      "Epoch:2172 3/3...  Loss: 0.2545\n",
      "Epoch:2173 3/3...  Loss: 0.3807\n",
      "Epoch:2174 3/3...  Loss: 0.3301\n",
      "Epoch:2175 3/3...  Loss: 0.1642\n",
      "Epoch:2176 3/3...  Loss: 0.2251\n",
      "Epoch:2177 3/3...  Loss: 0.2221\n",
      "Epoch:2178 3/3...  Loss: 0.1970\n",
      "Epoch:2179 3/3...  Loss: 0.2504\n",
      "Epoch:2180 3/3...  Loss: 0.3382\n",
      "Epoch:2181 3/3...  Loss: 0.2112\n",
      "Epoch:2182 3/3...  Loss: 0.2926\n",
      "Epoch:2183 3/3...  Loss: 0.5771\n",
      "Epoch:2184 3/3...  Loss: 0.4118\n",
      "Epoch:2185 3/3...  Loss: 0.1875\n",
      "Epoch:2186 3/3...  Loss: 0.2590\n",
      "Epoch:2187 3/3...  Loss: 0.2687\n",
      "Epoch:2188 3/3...  Loss: 0.2546\n",
      "Epoch:2189 3/3...  Loss: 0.2284\n",
      "Epoch:2190 3/3...  Loss: 0.3183\n",
      "Epoch:2191 3/3...  Loss: 0.3700\n",
      "Epoch:2192 3/3...  Loss: 0.1677\n",
      "Epoch:2193 3/3...  Loss: 0.3052\n",
      "Epoch:2194 3/3...  Loss: 0.4149\n",
      "Epoch:2195 3/3...  Loss: 0.6755\n",
      "Epoch:2196 3/3...  Loss: 0.4004\n",
      "Epoch:2197 3/3...  Loss: 0.2424\n",
      "Epoch:2198 3/3...  Loss: 0.2463\n",
      "Epoch:2199 3/3...  Loss: 0.3281\n",
      "Epoch:2200 3/3...  Loss: 0.3904\n",
      "Epoch:2201 3/3...  Loss: 0.3245\n",
      "Epoch:2202 3/3...  Loss: 0.3983\n",
      "Epoch:2203 3/3...  Loss: 0.3857\n",
      "Epoch:2204 3/3...  Loss: 0.4686\n",
      "Epoch:2205 3/3...  Loss: 0.5044\n",
      "Epoch:2206 3/3...  Loss: 0.3070\n",
      "Epoch:2207 3/3...  Loss: 0.3136\n",
      "Epoch:2208 3/3...  Loss: 0.4121\n",
      "Epoch:2209 3/3...  Loss: 0.3223\n",
      "Epoch:2210 3/3...  Loss: 0.3668\n",
      "Epoch:2211 3/3...  Loss: 0.2679\n",
      "Epoch:2212 3/3...  Loss: 0.2894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2213 3/3...  Loss: 0.3473\n",
      "Epoch:2214 3/3...  Loss: 0.2437\n",
      "Epoch:2215 3/3...  Loss: 0.4229\n",
      "Epoch:2216 3/3...  Loss: 0.3178\n",
      "Epoch:2217 3/3...  Loss: 0.5551\n",
      "Epoch:2218 3/3...  Loss: 0.2517\n",
      "Epoch:2219 3/3...  Loss: 0.4185\n",
      "Epoch:2220 3/3...  Loss: 0.3140\n",
      "Epoch:2221 3/3...  Loss: 0.3374\n",
      "Epoch:2222 3/3...  Loss: 0.1611\n",
      "Epoch:2223 3/3...  Loss: 0.3921\n",
      "Epoch:2224 3/3...  Loss: 0.3146\n",
      "Epoch:2225 3/3...  Loss: 0.2945\n",
      "Epoch:2226 3/3...  Loss: 0.2647\n",
      "Epoch:2227 3/3...  Loss: 0.2400\n",
      "Epoch:2228 3/3...  Loss: 0.2548\n",
      "Epoch:2229 3/3...  Loss: 0.1580\n",
      "Epoch:2230 3/3...  Loss: 0.4078\n",
      "Epoch:2231 3/3...  Loss: 0.3177\n",
      "Epoch:2232 3/3...  Loss: 0.3411\n",
      "Epoch:2233 3/3...  Loss: 0.4273\n",
      "Epoch:2234 3/3...  Loss: 0.2939\n",
      "Epoch:2235 3/3...  Loss: 0.2468\n",
      "Epoch:2236 3/3...  Loss: 0.2435\n",
      "Epoch:2237 3/3...  Loss: 0.3005\n",
      "Epoch:2238 3/3...  Loss: 0.4637\n",
      "Epoch:2239 3/3...  Loss: 0.3717\n",
      "Epoch:2240 3/3...  Loss: 0.2069\n",
      "Epoch:2241 3/3...  Loss: 0.1933\n",
      "Epoch:2242 3/3...  Loss: 0.3357\n",
      "Epoch:2243 3/3...  Loss: 0.2945\n",
      "Epoch:2244 3/3...  Loss: 0.3148\n",
      "Epoch:2245 3/3...  Loss: 0.2783\n",
      "Epoch:2246 3/3...  Loss: 0.4341\n",
      "Epoch:2247 3/3...  Loss: 0.3011\n",
      "Epoch:2248 3/3...  Loss: 0.2813\n",
      "Epoch:2249 3/3...  Loss: 0.2485\n",
      "Epoch:2250 3/3...  Loss: 0.2870\n",
      "Epoch:2251 3/3...  Loss: 0.2322\n",
      "Epoch:2252 3/3...  Loss: 0.3258\n",
      "Epoch:2253 3/3...  Loss: 0.1350\n",
      "Epoch:2254 3/3...  Loss: 0.2793\n",
      "Epoch:2255 3/3...  Loss: 0.3906\n",
      "Epoch:2256 3/3...  Loss: 0.4131\n",
      "Epoch:2257 3/3...  Loss: 0.3416\n",
      "Epoch:2258 3/3...  Loss: 0.3106\n",
      "Epoch:2259 3/3...  Loss: 0.4300\n",
      "Epoch:2260 3/3...  Loss: 0.3029\n",
      "Epoch:2261 3/3...  Loss: 0.2279\n",
      "Epoch:2262 3/3...  Loss: 0.5020\n",
      "Epoch:2263 3/3...  Loss: 0.3081\n",
      "Epoch:2264 3/3...  Loss: 0.2333\n",
      "Epoch:2265 3/3...  Loss: 0.4947\n",
      "Epoch:2266 3/3...  Loss: 0.1977\n",
      "Epoch:2267 3/3...  Loss: 0.2741\n",
      "Epoch:2268 3/3...  Loss: 0.2303\n",
      "Epoch:2269 3/3...  Loss: 0.2741\n",
      "Epoch:2270 3/3...  Loss: 0.3023\n",
      "Epoch:2271 3/3...  Loss: 0.2514\n",
      "Epoch:2272 3/3...  Loss: 0.3862\n",
      "Epoch:2273 3/3...  Loss: 0.3015\n",
      "Epoch:2274 3/3...  Loss: 0.4178\n",
      "Epoch:2275 3/3...  Loss: 0.3598\n",
      "Epoch:2276 3/3...  Loss: 0.2724\n",
      "Epoch:2277 3/3...  Loss: 0.2368\n",
      "Epoch:2278 3/3...  Loss: 0.5734\n",
      "Epoch:2279 3/3...  Loss: 0.3616\n",
      "Epoch:2280 3/3...  Loss: 0.3382\n",
      "Epoch:2281 3/3...  Loss: 0.5291\n",
      "Epoch:2282 3/3...  Loss: 0.3124\n",
      "Epoch:2283 3/3...  Loss: 0.2990\n",
      "Epoch:2284 3/3...  Loss: 0.2870\n",
      "Epoch:2285 3/3...  Loss: 0.4504\n",
      "Epoch:2286 3/3...  Loss: 0.3066\n",
      "Epoch:2287 3/3...  Loss: 0.4194\n",
      "Epoch:2288 3/3...  Loss: 0.4072\n",
      "Epoch:2289 3/3...  Loss: 0.3511\n",
      "Epoch:2290 3/3...  Loss: 0.1657\n",
      "Epoch:2291 3/3...  Loss: 0.3630\n",
      "Epoch:2292 3/3...  Loss: 0.3168\n",
      "Epoch:2293 3/3...  Loss: 0.3852\n",
      "Epoch:2294 3/3...  Loss: 0.4200\n",
      "Epoch:2295 3/3...  Loss: 0.2125\n",
      "Epoch:2296 3/3...  Loss: 0.2990\n",
      "Epoch:2297 3/3...  Loss: 0.3776\n",
      "Epoch:2298 3/3...  Loss: 0.2699\n",
      "Epoch:2299 3/3...  Loss: 0.6355\n",
      "Epoch:2300 3/3...  Loss: 0.2105\n",
      "Epoch:2301 3/3...  Loss: 0.1530\n",
      "Epoch:2302 3/3...  Loss: 0.3577\n",
      "Epoch:2303 3/3...  Loss: 0.6355\n",
      "Epoch:2304 3/3...  Loss: 0.3313\n",
      "Epoch:2305 3/3...  Loss: 0.5419\n",
      "Epoch:2306 3/3...  Loss: 0.2756\n",
      "Epoch:2307 3/3...  Loss: 0.1968\n",
      "Epoch:2308 3/3...  Loss: 0.3893\n",
      "Epoch:2309 3/3...  Loss: 0.1307\n",
      "Epoch:2310 3/3...  Loss: 0.2024\n",
      "Epoch:2311 3/3...  Loss: 0.2696\n",
      "Epoch:2312 3/3...  Loss: 0.3205\n",
      "Epoch:2313 3/3...  Loss: 0.2821\n",
      "Epoch:2314 3/3...  Loss: 0.1620\n",
      "Epoch:2315 3/3...  Loss: 0.3518\n",
      "Epoch:2316 3/3...  Loss: 0.2901\n",
      "Epoch:2317 3/3...  Loss: 0.2650\n",
      "Epoch:2318 3/3...  Loss: 0.4215\n",
      "Epoch:2319 3/3...  Loss: 0.4714\n",
      "Epoch:2320 3/3...  Loss: 0.4399\n",
      "Epoch:2321 3/3...  Loss: 0.3218\n",
      "Epoch:2322 3/3...  Loss: 0.2715\n",
      "Epoch:2323 3/3...  Loss: 0.3641\n",
      "Epoch:2324 3/3...  Loss: 0.2803\n",
      "Epoch:2325 3/3...  Loss: 0.3117\n",
      "Epoch:2326 3/3...  Loss: 0.4014\n",
      "Epoch:2327 3/3...  Loss: 0.2658\n",
      "Epoch:2328 3/3...  Loss: 0.2147\n",
      "Epoch:2329 3/3...  Loss: 0.2206\n",
      "Epoch:2330 3/3...  Loss: 0.1060\n",
      "Epoch:2331 3/3...  Loss: 0.2265\n",
      "Epoch:2332 3/3...  Loss: 0.4944\n",
      "Epoch:2333 3/3...  Loss: 0.1980\n",
      "Epoch:2334 3/3...  Loss: 0.4451\n",
      "Epoch:2335 3/3...  Loss: 0.1151\n",
      "Epoch:2336 3/3...  Loss: 0.3896\n",
      "Epoch:2337 3/3...  Loss: 0.2839\n",
      "Epoch:2338 3/3...  Loss: 0.4417\n",
      "Epoch:2339 3/3...  Loss: 0.1714\n",
      "Epoch:2340 3/3...  Loss: 0.2238\n",
      "Epoch:2341 3/3...  Loss: 0.2323\n",
      "Epoch:2342 3/3...  Loss: 0.4048\n",
      "Epoch:2343 3/3...  Loss: 0.2072\n",
      "Epoch:2344 3/3...  Loss: 0.2017\n",
      "Epoch:2345 3/3...  Loss: 0.1932\n",
      "Epoch:2346 3/3...  Loss: 0.2618\n",
      "Epoch:2347 3/3...  Loss: 0.3426\n",
      "Epoch:2348 3/3...  Loss: 0.1336\n",
      "Epoch:2349 3/3...  Loss: 0.1987\n",
      "Epoch:2350 3/3...  Loss: 0.5066\n",
      "Epoch:2351 3/3...  Loss: 0.2942\n",
      "Epoch:2352 3/3...  Loss: 0.6159\n",
      "Epoch:2353 3/3...  Loss: 0.1548\n",
      "Epoch:2354 3/3...  Loss: 0.4600\n",
      "Epoch:2355 3/3...  Loss: 0.3246\n",
      "Epoch:2356 3/3...  Loss: 0.3025\n",
      "Epoch:2357 3/3...  Loss: 0.2138\n",
      "Epoch:2358 3/3...  Loss: 0.2278\n",
      "Epoch:2359 3/3...  Loss: 0.3955\n",
      "Epoch:2360 3/3...  Loss: 0.2153\n",
      "Epoch:2361 3/3...  Loss: 0.3992\n",
      "Epoch:2362 3/3...  Loss: 0.3479\n",
      "Epoch:2363 3/3...  Loss: 0.2075\n",
      "Epoch:2364 3/3...  Loss: 0.3805\n",
      "Epoch:2365 3/3...  Loss: 0.1938\n",
      "Epoch:2366 3/3...  Loss: 0.1913\n",
      "Epoch:2367 3/3...  Loss: 0.2258\n",
      "Epoch:2368 3/3...  Loss: 0.3474\n",
      "Epoch:2369 3/3...  Loss: 0.3093\n",
      "Epoch:2370 3/3...  Loss: 0.2533\n",
      "Epoch:2371 3/3...  Loss: 0.3571\n",
      "Epoch:2372 3/3...  Loss: 0.4149\n",
      "Epoch:2373 3/3...  Loss: 0.1502\n",
      "Epoch:2374 3/3...  Loss: 0.3715\n",
      "Epoch:2375 3/3...  Loss: 0.4548\n",
      "Epoch:2376 3/3...  Loss: 0.2452\n",
      "Epoch:2377 3/3...  Loss: 0.3596\n",
      "Epoch:2378 3/3...  Loss: 0.3754\n",
      "Epoch:2379 3/3...  Loss: 0.2138\n",
      "Epoch:2380 3/3...  Loss: 0.2351\n",
      "Epoch:2381 3/3...  Loss: 0.3458\n",
      "Epoch:2382 3/3...  Loss: 0.3388\n",
      "Epoch:2383 3/3...  Loss: 0.3769\n",
      "Epoch:2384 3/3...  Loss: 0.3206\n",
      "Epoch:2385 3/3...  Loss: 0.2762\n",
      "Epoch:2386 3/3...  Loss: 0.3251\n",
      "Epoch:2387 3/3...  Loss: 0.4175\n",
      "Epoch:2388 3/3...  Loss: 0.4095\n",
      "Epoch:2389 3/3...  Loss: 0.2079\n",
      "Epoch:2390 3/3...  Loss: 0.1849\n",
      "Epoch:2391 3/3...  Loss: 0.3983\n",
      "Epoch:2392 3/3...  Loss: 0.1934\n",
      "Epoch:2393 3/3...  Loss: 0.3190\n",
      "Epoch:2394 3/3...  Loss: 0.2944\n",
      "Epoch:2395 3/3...  Loss: 0.1434\n",
      "Epoch:2396 3/3...  Loss: 0.3535\n",
      "Epoch:2397 3/3...  Loss: 0.4324\n",
      "Epoch:2398 3/3...  Loss: 0.3519\n",
      "Epoch:2399 3/3...  Loss: 0.3289\n",
      "Epoch:2400 3/3...  Loss: 0.1982\n",
      "Epoch:2401 3/3...  Loss: 0.3117\n",
      "Epoch:2402 3/3...  Loss: 0.5188\n",
      "Epoch:2403 3/3...  Loss: 0.3616\n",
      "Epoch:2404 3/3...  Loss: 0.2605\n",
      "Epoch:2405 3/3...  Loss: 0.4100\n",
      "Epoch:2406 3/3...  Loss: 0.3883\n",
      "Epoch:2407 3/3...  Loss: 0.1285\n",
      "Epoch:2408 3/3...  Loss: 0.2607\n",
      "Epoch:2409 3/3...  Loss: 0.3806\n",
      "Epoch:2410 3/3...  Loss: 0.5419\n",
      "Epoch:2411 3/3...  Loss: 0.5919\n",
      "Epoch:2412 3/3...  Loss: 0.3543\n",
      "Epoch:2413 3/3...  Loss: 0.2153\n",
      "Epoch:2414 3/3...  Loss: 0.4944\n",
      "Epoch:2415 3/3...  Loss: 0.2921\n",
      "Epoch:2416 3/3...  Loss: 0.1493\n",
      "Epoch:2417 3/3...  Loss: 0.5375\n",
      "Epoch:2418 3/3...  Loss: 0.3013\n",
      "Epoch:2419 3/3...  Loss: 0.4134\n",
      "Epoch:2420 3/3...  Loss: 0.3246\n",
      "Epoch:2421 3/3...  Loss: 0.3205\n",
      "Epoch:2422 3/3...  Loss: 0.2334\n",
      "Epoch:2423 3/3...  Loss: 0.1778\n",
      "Epoch:2424 3/3...  Loss: 0.2509\n",
      "Epoch:2425 3/3...  Loss: 0.2397\n",
      "Epoch:2426 3/3...  Loss: 0.1901\n",
      "Epoch:2427 3/3...  Loss: 0.2869\n",
      "Epoch:2428 3/3...  Loss: 0.2246\n",
      "Epoch:2429 3/3...  Loss: 0.4214\n",
      "Epoch:2430 3/3...  Loss: 0.1376\n",
      "Epoch:2431 3/3...  Loss: 0.3504\n",
      "Epoch:2432 3/3...  Loss: 0.2586\n",
      "Epoch:2433 3/3...  Loss: 0.1838\n",
      "Epoch:2434 3/3...  Loss: 0.1330\n",
      "Epoch:2435 3/3...  Loss: 0.1996\n",
      "Epoch:2436 3/3...  Loss: 0.1781\n",
      "Epoch:2437 3/3...  Loss: 0.3158\n",
      "Epoch:2438 3/3...  Loss: 0.4696\n",
      "Epoch:2439 3/3...  Loss: 0.1566\n",
      "Epoch:2440 3/3...  Loss: 0.3059\n",
      "Epoch:2441 3/3...  Loss: 0.3874\n",
      "Epoch:2442 3/3...  Loss: 0.2397\n",
      "Epoch:2443 3/3...  Loss: 0.2912\n",
      "Epoch:2444 3/3...  Loss: 0.3092\n",
      "Epoch:2445 3/3...  Loss: 0.2095\n",
      "Epoch:2446 3/3...  Loss: 0.4193\n",
      "Epoch:2447 3/3...  Loss: 0.2472\n",
      "Epoch:2448 3/3...  Loss: 0.2638\n",
      "Epoch:2449 3/3...  Loss: 0.4206\n",
      "Epoch:2450 3/3...  Loss: 0.3215\n",
      "Epoch:2451 3/3...  Loss: 0.3139\n",
      "Epoch:2452 3/3...  Loss: 0.2865\n",
      "Epoch:2453 3/3...  Loss: 0.3342\n",
      "Epoch:2454 3/3...  Loss: 0.2471\n",
      "Epoch:2455 3/3...  Loss: 0.2112\n",
      "Epoch:2456 3/3...  Loss: 0.2463\n",
      "Epoch:2457 3/3...  Loss: 0.4809\n",
      "Epoch:2458 3/3...  Loss: 0.4294\n",
      "Epoch:2459 3/3...  Loss: 0.1603\n",
      "Epoch:2460 3/3...  Loss: 0.4738\n",
      "Epoch:2461 3/3...  Loss: 0.7014\n",
      "Epoch:2462 3/3...  Loss: 0.3327\n",
      "Epoch:2463 3/3...  Loss: 0.5157\n",
      "Epoch:2464 3/3...  Loss: 0.3783\n",
      "Epoch:2465 3/3...  Loss: 0.3052\n",
      "Epoch:2466 3/3...  Loss: 0.2311\n",
      "Epoch:2467 3/3...  Loss: 0.2866\n",
      "Epoch:2468 3/3...  Loss: 0.5163\n",
      "Epoch:2469 3/3...  Loss: 0.2781\n",
      "Epoch:2470 3/3...  Loss: 0.5019\n",
      "Epoch:2471 3/3...  Loss: 0.1343\n",
      "Epoch:2472 3/3...  Loss: 0.3823\n",
      "Epoch:2473 3/3...  Loss: 0.3529\n",
      "Epoch:2474 3/3...  Loss: 0.2190\n",
      "Epoch:2475 3/3...  Loss: 0.3830\n",
      "Epoch:2476 3/3...  Loss: 0.3559\n",
      "Epoch:2477 3/3...  Loss: 0.2404\n",
      "Epoch:2478 3/3...  Loss: 0.2749\n",
      "Epoch:2479 3/3...  Loss: 0.3835\n",
      "Epoch:2480 3/3...  Loss: 0.3030\n",
      "Epoch:2481 3/3...  Loss: 0.2211\n",
      "Epoch:2482 3/3...  Loss: 0.1414\n",
      "Epoch:2483 3/3...  Loss: 0.2194\n",
      "Epoch:2484 3/3...  Loss: 0.6096\n",
      "Epoch:2485 3/3...  Loss: 0.2865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2486 3/3...  Loss: 0.2607\n",
      "Epoch:2487 3/3...  Loss: 0.3341\n",
      "Epoch:2488 3/3...  Loss: 0.2643\n",
      "Epoch:2489 3/3...  Loss: 0.3800\n",
      "Epoch:2490 3/3...  Loss: 0.4084\n",
      "Epoch:2491 3/3...  Loss: 0.2940\n",
      "Epoch:2492 3/3...  Loss: 0.3467\n",
      "Epoch:2493 3/3...  Loss: 0.2163\n",
      "Epoch:2494 3/3...  Loss: 0.2208\n",
      "Epoch:2495 3/3...  Loss: 0.4125\n",
      "Epoch:2496 3/3...  Loss: 0.4908\n",
      "Epoch:2497 3/3...  Loss: 0.5027\n",
      "Epoch:2498 3/3...  Loss: 0.2790\n",
      "Epoch:2499 3/3...  Loss: 0.3280\n",
      "Epoch:2500 3/3...  Loss: 0.1959\n",
      "Epoch:2501 3/3...  Loss: 0.3288\n",
      "Epoch:2502 3/3...  Loss: 0.3235\n",
      "Epoch:2503 3/3...  Loss: 0.4001\n",
      "Epoch:2504 3/3...  Loss: 0.3161\n",
      "Epoch:2505 3/3...  Loss: 0.4204\n",
      "Epoch:2506 3/3...  Loss: 0.1414\n",
      "Epoch:2507 3/3...  Loss: 0.1736\n",
      "Epoch:2508 3/3...  Loss: 0.4220\n",
      "Epoch:2509 3/3...  Loss: 0.1899\n",
      "Epoch:2510 3/3...  Loss: 0.1924\n",
      "Epoch:2511 3/3...  Loss: 0.5714\n",
      "Epoch:2512 3/3...  Loss: 0.2106\n",
      "Epoch:2513 3/3...  Loss: 0.2815\n",
      "Epoch:2514 3/3...  Loss: 0.1889\n",
      "Epoch:2515 3/3...  Loss: 0.4067\n",
      "Epoch:2516 3/3...  Loss: 0.3313\n",
      "Epoch:2517 3/3...  Loss: 0.2163\n",
      "Epoch:2518 3/3...  Loss: 0.2933\n",
      "Epoch:2519 3/3...  Loss: 0.1280\n",
      "Epoch:2520 3/3...  Loss: 0.3248\n",
      "Epoch:2521 3/3...  Loss: 0.2239\n",
      "Epoch:2522 3/3...  Loss: 0.3953\n",
      "Epoch:2523 3/3...  Loss: 0.3668\n",
      "Epoch:2524 3/3...  Loss: 0.4415\n",
      "Epoch:2525 3/3...  Loss: 0.2606\n",
      "Epoch:2526 3/3...  Loss: 0.2869\n",
      "Epoch:2527 3/3...  Loss: 0.2718\n",
      "Epoch:2528 3/3...  Loss: 0.4175\n",
      "Epoch:2529 3/3...  Loss: 0.2997\n",
      "Epoch:2530 3/3...  Loss: 0.4045\n",
      "Epoch:2531 3/3...  Loss: 0.3532\n",
      "Epoch:2532 3/3...  Loss: 0.3540\n",
      "Epoch:2533 3/3...  Loss: 0.5446\n",
      "Epoch:2534 3/3...  Loss: 0.3256\n",
      "Epoch:2535 3/3...  Loss: 0.2615\n",
      "Epoch:2536 3/3...  Loss: 0.4654\n",
      "Epoch:2537 3/3...  Loss: 0.2229\n",
      "Epoch:2538 3/3...  Loss: 0.3083\n",
      "Epoch:2539 3/3...  Loss: 0.2623\n",
      "Epoch:2540 3/3...  Loss: 0.2933\n",
      "Epoch:2541 3/3...  Loss: 0.3672\n",
      "Epoch:2542 3/3...  Loss: 0.3308\n",
      "Epoch:2543 3/3...  Loss: 0.1721\n",
      "Epoch:2544 3/3...  Loss: 0.3368\n",
      "Epoch:2545 3/3...  Loss: 0.2599\n",
      "Epoch:2546 3/3...  Loss: 0.2971\n",
      "Epoch:2547 3/3...  Loss: 0.2125\n",
      "Epoch:2548 3/3...  Loss: 0.1999\n",
      "Epoch:2549 3/3...  Loss: 0.2198\n",
      "Epoch:2550 3/3...  Loss: 0.2453\n",
      "Epoch:2551 3/3...  Loss: 0.4944\n",
      "Epoch:2552 3/3...  Loss: 0.3096\n",
      "Epoch:2553 3/3...  Loss: 0.2947\n",
      "Epoch:2554 3/3...  Loss: 0.3445\n",
      "Epoch:2555 3/3...  Loss: 0.5164\n",
      "Epoch:2556 3/3...  Loss: 0.2496\n",
      "Epoch:2557 3/3...  Loss: 0.4503\n",
      "Epoch:2558 3/3...  Loss: 0.3265\n",
      "Epoch:2559 3/3...  Loss: 0.1658\n",
      "Epoch:2560 3/3...  Loss: 0.1790\n",
      "Epoch:2561 3/3...  Loss: 0.3461\n",
      "Epoch:2562 3/3...  Loss: 0.1248\n",
      "Epoch:2563 3/3...  Loss: 0.1414\n",
      "Epoch:2564 3/3...  Loss: 0.1976\n",
      "Epoch:2565 3/3...  Loss: 0.4257\n",
      "Epoch:2566 3/3...  Loss: 0.3126\n",
      "Epoch:2567 3/3...  Loss: 0.3929\n",
      "Epoch:2568 3/3...  Loss: 0.2015\n",
      "Epoch:2569 3/3...  Loss: 0.4129\n",
      "Epoch:2570 3/3...  Loss: 0.4134\n",
      "Epoch:2571 3/3...  Loss: 0.4706\n",
      "Epoch:2572 3/3...  Loss: 0.3078\n",
      "Epoch:2573 3/3...  Loss: 0.3722\n",
      "Epoch:2574 3/3...  Loss: 0.4396\n",
      "Epoch:2575 3/3...  Loss: 0.2641\n",
      "Epoch:2576 3/3...  Loss: 0.4289\n",
      "Epoch:2577 3/3...  Loss: 0.1957\n",
      "Epoch:2578 3/3...  Loss: 0.2930\n",
      "Epoch:2579 3/3...  Loss: 0.1844\n",
      "Epoch:2580 3/3...  Loss: 0.4842\n",
      "Epoch:2581 3/3...  Loss: 0.4029\n",
      "Epoch:2582 3/3...  Loss: 0.3211\n",
      "Epoch:2583 3/3...  Loss: 0.2052\n",
      "Epoch:2584 3/3...  Loss: 0.3397\n",
      "Epoch:2585 3/3...  Loss: 0.4899\n",
      "Epoch:2586 3/3...  Loss: 0.3462\n",
      "Epoch:2587 3/3...  Loss: 0.1956\n",
      "Epoch:2588 3/3...  Loss: 0.2999\n",
      "Epoch:2589 3/3...  Loss: 0.3419\n",
      "Epoch:2590 3/3...  Loss: 0.2237\n",
      "Epoch:2591 3/3...  Loss: 0.3172\n",
      "Epoch:2592 3/3...  Loss: 0.2970\n",
      "Epoch:2593 3/3...  Loss: 0.3507\n",
      "Epoch:2594 3/3...  Loss: 0.2672\n",
      "Epoch:2595 3/3...  Loss: 0.4180\n",
      "Epoch:2596 3/3...  Loss: 0.2708\n",
      "Epoch:2597 3/3...  Loss: 0.2589\n",
      "Epoch:2598 3/3...  Loss: 0.2606\n",
      "Epoch:2599 3/3...  Loss: 0.2068\n",
      "Epoch:2600 3/3...  Loss: 0.1678\n",
      "Epoch:2601 3/3...  Loss: 0.2314\n",
      "Epoch:2602 3/3...  Loss: 0.3339\n",
      "Epoch:2603 3/3...  Loss: 0.2607\n",
      "Epoch:2604 3/3...  Loss: 0.3106\n",
      "Epoch:2605 3/3...  Loss: 0.2612\n",
      "Epoch:2606 3/3...  Loss: 0.3516\n",
      "Epoch:2607 3/3...  Loss: 0.4190\n",
      "Epoch:2608 3/3...  Loss: 0.3085\n",
      "Epoch:2609 3/3...  Loss: 0.3060\n",
      "Epoch:2610 3/3...  Loss: 0.6096\n",
      "Epoch:2611 3/3...  Loss: 0.2455\n",
      "Epoch:2612 3/3...  Loss: 0.3829\n",
      "Epoch:2613 3/3...  Loss: 0.2306\n",
      "Epoch:2614 3/3...  Loss: 0.3772\n",
      "Epoch:2615 3/3...  Loss: 0.3323\n",
      "Epoch:2616 3/3...  Loss: 0.4285\n",
      "Epoch:2617 3/3...  Loss: 0.3496\n",
      "Epoch:2618 3/3...  Loss: 0.6371\n",
      "Epoch:2619 3/3...  Loss: 0.2439\n",
      "Epoch:2620 3/3...  Loss: 0.3485\n",
      "Epoch:2621 3/3...  Loss: 0.1548\n",
      "Epoch:2622 3/3...  Loss: 0.3135\n",
      "Epoch:2623 3/3...  Loss: 0.2190\n",
      "Epoch:2624 3/3...  Loss: 0.3157\n",
      "Epoch:2625 3/3...  Loss: 0.4618\n",
      "Epoch:2626 3/3...  Loss: 0.3406\n",
      "Epoch:2627 3/3...  Loss: 0.2476\n",
      "Epoch:2628 3/3...  Loss: 0.3438\n",
      "Epoch:2629 3/3...  Loss: 0.3745\n",
      "Epoch:2630 3/3...  Loss: 0.2120\n",
      "Epoch:2631 3/3...  Loss: 0.1931\n",
      "Epoch:2632 3/3...  Loss: 0.2994\n",
      "Epoch:2633 3/3...  Loss: 0.3887\n",
      "Epoch:2634 3/3...  Loss: 0.3157\n",
      "Epoch:2635 3/3...  Loss: 0.2071\n",
      "Epoch:2636 3/3...  Loss: 0.1298\n",
      "Epoch:2637 3/3...  Loss: 0.2492\n",
      "Epoch:2638 3/3...  Loss: 0.6403\n",
      "Epoch:2639 3/3...  Loss: 0.3653\n",
      "Epoch:2640 3/3...  Loss: 0.2094\n",
      "Epoch:2641 3/3...  Loss: 0.5253\n",
      "Epoch:2642 3/3...  Loss: 0.5325\n",
      "Epoch:2643 3/3...  Loss: 0.2074\n",
      "Epoch:2644 3/3...  Loss: 0.3883\n",
      "Epoch:2645 3/3...  Loss: 0.5793\n",
      "Epoch:2646 3/3...  Loss: 0.2956\n",
      "Epoch:2647 3/3...  Loss: 0.1261\n",
      "Epoch:2648 3/3...  Loss: 0.2579\n",
      "Epoch:2649 3/3...  Loss: 0.2220\n",
      "Epoch:2650 3/3...  Loss: 0.2364\n",
      "Epoch:2651 3/3...  Loss: 0.4361\n",
      "Epoch:2652 3/3...  Loss: 0.4302\n",
      "Epoch:2653 3/3...  Loss: 0.3848\n",
      "Epoch:2654 3/3...  Loss: 0.3729\n",
      "Epoch:2655 3/3...  Loss: 0.3039\n",
      "Epoch:2656 3/3...  Loss: 0.3835\n",
      "Epoch:2657 3/3...  Loss: 0.3941\n",
      "Epoch:2658 3/3...  Loss: 0.2863\n",
      "Epoch:2659 3/3...  Loss: 0.1841\n",
      "Epoch:2660 3/3...  Loss: 0.2559\n",
      "Epoch:2661 3/3...  Loss: 0.3883\n",
      "Epoch:2662 3/3...  Loss: 0.2195\n",
      "Epoch:2663 3/3...  Loss: 0.4015\n",
      "Epoch:2664 3/3...  Loss: 0.4084\n",
      "Epoch:2665 3/3...  Loss: 0.2245\n",
      "Epoch:2666 3/3...  Loss: 0.2476\n",
      "Epoch:2667 3/3...  Loss: 0.5654\n",
      "Epoch:2668 3/3...  Loss: 0.2412\n",
      "Epoch:2669 3/3...  Loss: 0.3502\n",
      "Epoch:2670 3/3...  Loss: 0.2738\n",
      "Epoch:2671 3/3...  Loss: 0.4804\n",
      "Epoch:2672 3/3...  Loss: 0.2538\n",
      "Epoch:2673 3/3...  Loss: 0.3085\n",
      "Epoch:2674 3/3...  Loss: 0.4083\n",
      "Epoch:2675 3/3...  Loss: 0.2400\n",
      "Epoch:2676 3/3...  Loss: 0.4194\n",
      "Epoch:2677 3/3...  Loss: 0.3556\n",
      "Epoch:2678 3/3...  Loss: 0.3415\n",
      "Epoch:2679 3/3...  Loss: 0.5198\n",
      "Epoch:2680 3/3...  Loss: 0.3456\n",
      "Epoch:2681 3/3...  Loss: 0.4509\n",
      "Epoch:2682 3/3...  Loss: 0.2064\n",
      "Epoch:2683 3/3...  Loss: 0.2894\n",
      "Epoch:2684 3/3...  Loss: 0.2317\n",
      "Epoch:2685 3/3...  Loss: 0.3185\n",
      "Epoch:2686 3/3...  Loss: 0.5125\n",
      "Epoch:2687 3/3...  Loss: 0.2911\n",
      "Epoch:2688 3/3...  Loss: 0.3795\n",
      "Epoch:2689 3/3...  Loss: 0.1779\n",
      "Epoch:2690 3/3...  Loss: 0.4538\n",
      "Epoch:2691 3/3...  Loss: 0.2040\n",
      "Epoch:2692 3/3...  Loss: 0.2350\n",
      "Epoch:2693 3/3...  Loss: 0.3017\n",
      "Epoch:2694 3/3...  Loss: 0.3054\n",
      "Epoch:2695 3/3...  Loss: 0.2498\n",
      "Epoch:2696 3/3...  Loss: 0.2067\n",
      "Epoch:2697 3/3...  Loss: 0.4859\n",
      "Epoch:2698 3/3...  Loss: 0.3776\n",
      "Epoch:2699 3/3...  Loss: 0.2361\n",
      "Epoch:2700 3/3...  Loss: 0.1096\n",
      "Epoch:2701 3/3...  Loss: 0.2629\n",
      "Epoch:2702 3/3...  Loss: 0.1920\n",
      "Epoch:2703 3/3...  Loss: 0.3154\n",
      "Epoch:2704 3/3...  Loss: 0.3284\n",
      "Epoch:2705 3/3...  Loss: 0.3713\n",
      "Epoch:2706 3/3...  Loss: 0.3328\n",
      "Epoch:2707 3/3...  Loss: 0.3242\n",
      "Epoch:2708 3/3...  Loss: 0.4329\n",
      "Epoch:2709 3/3...  Loss: 0.3427\n",
      "Epoch:2710 3/3...  Loss: 0.3434\n",
      "Epoch:2711 3/3...  Loss: 0.2582\n",
      "Epoch:2712 3/3...  Loss: 0.4303\n",
      "Epoch:2713 3/3...  Loss: 0.2573\n",
      "Epoch:2714 3/3...  Loss: 0.3083\n",
      "Epoch:2715 3/3...  Loss: 0.1990\n",
      "Epoch:2716 3/3...  Loss: 0.2526\n",
      "Epoch:2717 3/3...  Loss: 0.3433\n",
      "Epoch:2718 3/3...  Loss: 0.3878\n",
      "Epoch:2719 3/3...  Loss: 0.3583\n",
      "Epoch:2720 3/3...  Loss: 0.2383\n",
      "Epoch:2721 3/3...  Loss: 0.2235\n",
      "Epoch:2722 3/3...  Loss: 0.2399\n",
      "Epoch:2723 3/3...  Loss: 0.1773\n",
      "Epoch:2724 3/3...  Loss: 0.2267\n",
      "Epoch:2725 3/3...  Loss: 0.2420\n",
      "Epoch:2726 3/3...  Loss: 0.1904\n",
      "Epoch:2727 3/3...  Loss: 0.3066\n",
      "Epoch:2728 3/3...  Loss: 0.3293\n",
      "Epoch:2729 3/3...  Loss: 0.2582\n",
      "Epoch:2730 3/3...  Loss: 0.4271\n",
      "Epoch:2731 3/3...  Loss: 0.3213\n",
      "Epoch:2732 3/3...  Loss: 0.3307\n",
      "Epoch:2733 3/3...  Loss: 0.3896\n",
      "Epoch:2734 3/3...  Loss: 0.3023\n",
      "Epoch:2735 3/3...  Loss: 0.2535\n",
      "Epoch:2736 3/3...  Loss: 0.3762\n",
      "Epoch:2737 3/3...  Loss: 0.2659\n",
      "Epoch:2738 3/3...  Loss: 0.3660\n",
      "Epoch:2739 3/3...  Loss: 0.2521\n",
      "Epoch:2740 3/3...  Loss: 0.3157\n",
      "Epoch:2741 3/3...  Loss: 0.3316\n",
      "Epoch:2742 3/3...  Loss: 0.2269\n",
      "Epoch:2743 3/3...  Loss: 0.2246\n",
      "Epoch:2744 3/3...  Loss: 0.3434\n",
      "Epoch:2745 3/3...  Loss: 0.3199\n",
      "Epoch:2746 3/3...  Loss: 0.2836\n",
      "Epoch:2747 3/3...  Loss: 0.5132\n",
      "Epoch:2748 3/3...  Loss: 0.2717\n",
      "Epoch:2749 3/3...  Loss: 0.2822\n",
      "Epoch:2750 3/3...  Loss: 0.3473\n",
      "Epoch:2751 3/3...  Loss: 0.2428\n",
      "Epoch:2752 3/3...  Loss: 0.1995\n",
      "Epoch:2753 3/3...  Loss: 0.3672\n",
      "Epoch:2754 3/3...  Loss: 0.3934\n",
      "Epoch:2755 3/3...  Loss: 0.1679\n",
      "Epoch:2756 3/3...  Loss: 0.2182\n",
      "Epoch:2757 3/3...  Loss: 0.3126\n",
      "Epoch:2758 3/3...  Loss: 0.2443\n",
      "Epoch:2759 3/3...  Loss: 0.3298\n",
      "Epoch:2760 3/3...  Loss: 0.2789\n",
      "Epoch:2761 3/3...  Loss: 0.2033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2762 3/3...  Loss: 0.5664\n",
      "Epoch:2763 3/3...  Loss: 0.2484\n",
      "Epoch:2764 3/3...  Loss: 0.2280\n",
      "Epoch:2765 3/3...  Loss: 0.3743\n",
      "Epoch:2766 3/3...  Loss: 0.4211\n",
      "Epoch:2767 3/3...  Loss: 0.4592\n",
      "Epoch:2768 3/3...  Loss: 0.2800\n",
      "Epoch:2769 3/3...  Loss: 0.2813\n",
      "Epoch:2770 3/3...  Loss: 0.3275\n",
      "Epoch:2771 3/3...  Loss: 0.1997\n",
      "Epoch:2772 3/3...  Loss: 0.4992\n",
      "Epoch:2773 3/3...  Loss: 0.2724\n",
      "Epoch:2774 3/3...  Loss: 0.2453\n",
      "Epoch:2775 3/3...  Loss: 0.5696\n",
      "Epoch:2776 3/3...  Loss: 0.4365\n",
      "Epoch:2777 3/3...  Loss: 0.2876\n",
      "Epoch:2778 3/3...  Loss: 0.3792\n",
      "Epoch:2779 3/3...  Loss: 0.2512\n",
      "Epoch:2780 3/3...  Loss: 0.4945\n",
      "Epoch:2781 3/3...  Loss: 0.3015\n",
      "Epoch:2782 3/3...  Loss: 0.2774\n",
      "Epoch:2783 3/3...  Loss: 0.3634\n",
      "Epoch:2784 3/3...  Loss: 0.2775\n",
      "Epoch:2785 3/3...  Loss: 0.3091\n",
      "Epoch:2786 3/3...  Loss: 0.5771\n",
      "Epoch:2787 3/3...  Loss: 0.3361\n",
      "Epoch:2788 3/3...  Loss: 0.2591\n",
      "Epoch:2789 3/3...  Loss: 0.3614\n",
      "Epoch:2790 3/3...  Loss: 0.3031\n",
      "Epoch:2791 3/3...  Loss: 0.3165\n",
      "Epoch:2792 3/3...  Loss: 0.2112\n",
      "Epoch:2793 3/3...  Loss: 0.2768\n",
      "Epoch:2794 3/3...  Loss: 0.2441\n",
      "Epoch:2795 3/3...  Loss: 0.2514\n",
      "Epoch:2796 3/3...  Loss: 0.4117\n",
      "Epoch:2797 3/3...  Loss: 0.2712\n",
      "Epoch:2798 3/3...  Loss: 0.2137\n",
      "Epoch:2799 3/3...  Loss: 0.3250\n",
      "Epoch:2800 3/3...  Loss: 0.4025\n",
      "Epoch:2801 3/3...  Loss: 0.1786\n",
      "Epoch:2802 3/3...  Loss: 0.2196\n",
      "Epoch:2803 3/3...  Loss: 0.3248\n",
      "Epoch:2804 3/3...  Loss: 0.2658\n",
      "Epoch:2805 3/3...  Loss: 0.4562\n",
      "Epoch:2806 3/3...  Loss: 0.4454\n",
      "Epoch:2807 3/3...  Loss: 0.3510\n",
      "Epoch:2808 3/3...  Loss: 0.3714\n",
      "Epoch:2809 3/3...  Loss: 0.3191\n",
      "Epoch:2810 3/3...  Loss: 0.4519\n",
      "Epoch:2811 3/3...  Loss: 0.4317\n",
      "Epoch:2812 3/3...  Loss: 0.3684\n",
      "Epoch:2813 3/3...  Loss: 0.2759\n",
      "Epoch:2814 3/3...  Loss: 0.1990\n"
     ]
    }
   ],
   "source": [
    "#to control data loading in each epoch instead of limiting it to 938:\n",
    "# nsamples = 10000\n",
    "# for i, image, label in enumerate(train_loader):\n",
    "#     if i > nsamples:\n",
    "#         break\n",
    "\n",
    "\n",
    "epochs = 3\n",
    "print_every = 1\n",
    "steps = 0\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in iter(trainloader):   #MNIST data is 60K test and 10K test. 64 (batch)*938 step~60K, since it shuffle should not be covering everythong probably\n",
    "        steps += 1\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward and backward passes\n",
    "        output = model.forward(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()  #loss is scalar tensor, to get it summ with running loss need to get loss.item()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            print(\"Epoch:{} {}/{}... \".format(steps,e+1, epochs),\n",
    "                  \"Loss: {:.4f}\".format(running_loss/print_every))\n",
    "            \n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the network trained, we can check out it's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 224,
       "width": 423
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logits = model.forward(img)\n",
    "\n",
    "# Output of the network are logits, need to take softmax for probabilities\n",
    "ps = F.softmax(logits, dim=1)\n",
    "helper.view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our network is brilliant. It can accurately predict the digits in our images. Next up you'll write the code for training a neural network on a more complex dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
